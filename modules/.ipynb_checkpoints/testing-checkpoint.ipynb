{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules and config import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, gzip, json\n",
    "from multiprocessing import Pool, Manager\n",
    "from itertools import product\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "\n",
    "from specieinfo import assemblies\n",
    "from specieinfo import specieinfo as si\n",
    "from download import download as dl\n",
    "from mainisoforms import mainisoforms as mi\n",
    "from duplications import duplications as dup\n",
    "from orthology import ortho as ort \n",
    "from faforalignments import faforalignments as ffal\n",
    "from database import database as db\n",
    "from database import dbinfo\n",
    "from alignments import alignments as al\n",
    "from features import features as feat\n",
    "\n",
    "cwd = os.path.dirname(os.getcwd())\n",
    "sys.path.append(cwd)\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(cwd + '/appris')\n",
    "os.mkdir(cwd + '/alignments')\n",
    "os.mkdir(cwd + '/alignments/tandem')\n",
    "os.mkdir(cwd + '/alignments/divergent')\n",
    "os.mkdir(cwd + '/alignments/convergent')\n",
    "os.mkdir(cwd + '/blast_queries')\n",
    "os.mkdir(cwd + '/duplications')\n",
    "os.mkdir(cwd + '/fa')\n",
    "os.mkdir(cwd + '/gtf')\n",
    "os.mkdir(cwd + '/main')\n",
    "os.mkdir(cwd + '/orthologues')\n",
    "os.mkdir(cwd + '/features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Species info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not have_list:\n",
    "\n",
    "    species = [a['name'] for a in assemblies['species']]       # collect all available species name in Ensembl database\n",
    "    slist = [si(s).allinfo for s in species]                   # specieinfo class for each of those species name\n",
    "\n",
    "    df = pd.DataFrame(slist, columns=[\n",
    "                                    'Class',\n",
    "                                    'Order',\n",
    "                                    'Genus',\n",
    "                                    'Specie',\n",
    "                                    'Publications',\n",
    "                                    'Taxid',\n",
    "                                    'Assembly']).dropna()      # writing a dataframe with all collected informations\n",
    "    df = df[df['Class'].isin(rf)]                              # filtering the dataframe for classes specified withing the config file\n",
    "    df['Publications'] = df['Publications'].astype(int)\n",
    "    df['Genus'] = df['Genus'].str.capitalize()                 # capitalize genus value\n",
    "    df = df.sort_values('Publications', ascending=False)       # sorting for the publications\n",
    "    if mo:\n",
    "        df = df.groupby('Order').head(mo)                      # limiting the max number of orders to collect (specified in config)\n",
    "    if mg:\n",
    "        df = df.groupby('Genus', sort=False).head(mg)          # same for the genres\n",
    "    df = df.sort_values(['Class', 'Order'])                    # last sorting for the final dataframe\n",
    "    df.to_csv(cwd + '/species/species_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. FASTAs and GTFs download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = pd.read_csv(cwd + '/species/species_list.csv')            # open species containing file\n",
    "fa = [l[2] + '_' + l[3] for l in ls.values.tolist()]           # write a list containing all the specie names for the fasta files\n",
    "gtf = rs                                                       # write a list containing all the specie names for the gtf files\n",
    "\n",
    "pool = Pool(threads)                                           # activating the dl function in multiprocessing\n",
    "pool.starmap(dl.downl, product(gtf, ['gtf']))                  # starmap multiprocessing accepts tuple with arguments [('Homo_sapiens', 'gtf'), ('Gallus_gallus', 'gtf'), etc...]\n",
    "pool.starmap(dl.downl, product(fa, ['fa']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Main isoforms and BLASTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainblast(s, n, e, h, t):\n",
    "    mi.tofa(s)                                                 # activating tofa function, write a principal isoforms containing .fa file\n",
    "    mi.totsv(s)                                                # activating totsv function, write a principal isoforms, sorted over the genome, containing .tsv file\n",
    "    mi.blast(s, n, e, h, t)                                    # performing an intraspecie blastP\n",
    "\n",
    "blast_args = [num_threads,\n",
    "              evalue,\n",
    "              max_hsps,\n",
    "              max_target_seqs]\n",
    "\n",
    "pool = Pool(threads)                                           # activating the dl function in multiprocessing\n",
    "args = [tuple([s] + blast_args) for s in rs]\n",
    "pool.starmap(mainblast, args)                                  # starmap multiprocessing accepts tuple with arguments [('Homo_sapiens', 20, '10e-6', 1, 5), ('Gallus_gallus', 20, '10e-6', 1, 5), etc...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Duplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tocsv(s, k):                                               # saving a .tsv file containing a list of duplications filtered for specie e duplication kind\n",
    "\n",
    "    dups = dup.duplist(s, k)                                   # retrieving the accessions list for specie and duplication kind\n",
    "    for l in dups:\n",
    "        path = cwd + '/duplications/'\n",
    "        file = path + s + '_' + k + '.tsv'\n",
    "        print(*l, sep='\\t', file=open(file, 'a'))              # writing duplicated pair in a tsv file\n",
    "\n",
    "Pool(threads).starmap(tocsv, product(rs, dups))                # activating the tocsv function in multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Orthology (COMPARA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forquery(ID, d):\n",
    "    \n",
    "    file = d + '_no_filter.csv'\n",
    "    path = cwd + '/orthologues/' + file\n",
    "    df = ort.df(ID[0], True)                                   # per ogni ID fornito scarica la lista degli ortologhi in formato dataframe, se viene fornita una lista di specie viene filtrato per queste\n",
    "    df['orthogroup'] = ID[1]                                   # assegna un numero corrispondende all'ortogruppo in base all'ordine di apparizione nel genoma dell'ID in esame\n",
    "    df = df.values.tolist()\n",
    "\n",
    "    for l in df:\n",
    "        print(*l, sep=';', file=open(path, 'a'))               # dopo la conversione in lista delle righe del dataframe, vengono stampati al momento i risultati in un file con tutte le informazioni non filtrate\n",
    "        \n",
    "def forspecie(rs, d):\n",
    "        \n",
    "    dups_df = ort.mergedups(rs, d)[[8, 10]]                    # unisce i dataframe contenenti i geni duplicati di tutte le specie di riferimento per ciascun tipo di duplicazione\n",
    "    IDS = [l for l in dups_df.values.tolist()]                 # recupera tutti gli ID e i numeri degli ortogruppi assegnati per ogni tipo di duplicazione\n",
    "    Pool(5).starmap(forquery, product(IDS, [d]))               # attiva la funzione (in multiprocessing) forquery per ogni ID presente nel dataframe nato dall'unione precedente\n",
    "\n",
    "dups = ['convergent', 'divergent']\n",
    "for d in dups:\n",
    "    file = d + '_no_filter.csv'\n",
    "    path = cwd + '/orthologues/' + file\n",
    "    forspecie(rs, d)                                           # attiva la funzione for specie per ogni specie di riferimento e tipo di duplicazione\n",
    "    ort.brh(path)                                              # brh interno nel dataframe di ortologia ottenuto, altri commenti presenti in orthology.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db.orthodf('Tandem')                                      # opening orthologues dataframe for:\n",
    "slist = df['Species'].values.tolist()                          # obtaining species list\n",
    "\n",
    "manager = Manager()\n",
    "data = manager.dict()\n",
    "\n",
    "def db_func(s):\n",
    " \n",
    "    path = cwd + '/fa/' + s + '.fa.gz'\n",
    "    handle = gzip.open(path, 'rt')\n",
    "    fastas = list(SeqIO.parse(handle, \"fasta\"))                # opening file.fa --> storing in a list\n",
    "\n",
    "    orthos = db.aclist(s)                                      # accessions list from the orthologues tab\n",
    "    f = [l for l in fastas \n",
    "         if l.id.split('.')[0] in orthos]                      # intersection between orthologues accession list and fastas\n",
    "\n",
    "    for l in f:\n",
    "        data.update(db.info(l))                                # activating database class info function\n",
    "        \n",
    "Pool(threads).map(db_func, \n",
    "    [s for s in slist])\n",
    "        \n",
    "json.dump(data.copy(), open(cwd + '/database.json', 'w'))      # dumping json database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. FASTAs for alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dups:                                                 # for each kind of duplication\n",
    "\n",
    "    df = db.orthodf(d)                                         #Â open and keep in memory the orthologues dataframe\n",
    "    pairs = ffal.pairslist(d)                                  # write a list containing the orthologues dataframe column indexes corresponding to orthogroup pairs \n",
    "    suff = db.suffixes()                                       # write a dictionary containing the species references based on accessions Ensembl coding {ENSP0: 'Homo_sapiens'}\n",
    "    js = json.load(open(cwd + '/database.json'))               # open the local database wrote in step 6\n",
    "    \n",
    "    for p in pairs:                                            # iterating over dataframe column indexes\n",
    "        ffal.printfa(df, p, suff, js, d)                       # writing FASTA file (1.fa will contain the FASTA corresponding to columns 1A and 1B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dups:\n",
    "    \n",
    "    folder = 'alignments/' + d\n",
    "    ogroups = al.fanum(folder, '.fa')\n",
    "    \n",
    "    for o in ogroups:\n",
    "        \n",
    "        fa = 'alignments/' + d + '/' + str(o) + '.fa'\n",
    "        faln = 'alignments/' + d + '/' + str(o) + '.fasta'\n",
    "        al.clustifcov(fa, faln, th_coverage)              \n",
    "        \n",
    "    manager = Manager()\n",
    "    logs = manager.dict()\n",
    "    \n",
    "    def log(ogroup, srefs, matrix):\n",
    "        \n",
    "        try:\n",
    "            faln = folder + '/' + str(ogroup) + '.fasta'\n",
    "            falnfile = al.alignmentfile(faln)\n",
    "            if not ogroup in logs.keys():\n",
    "                log = al.log(falnfile, srefs, matrix)\n",
    "                logs.update({ogroup: log})\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    Pool(threads).starmap(log, product(ogroups, [srefs], [matrix]))\n",
    "        \n",
    "    json.dump(logs.copy(), open(cwd + '/' + folder + '/' + d + '.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-7:\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/carloderito/anaconda3/envs/myenv3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "dups = ['convergent']\n",
    "for d in dups:\n",
    "\n",
    "    ortholist = dbinfo.ids_to(                                     # IDs conversion from protein to gene \n",
    "        features_ref_specie, d, 'gene')\n",
    "    converted = feat.convert_id(                                   # IDs conversion from ENSEMBL to Uniprot\n",
    "        'ENSEMBL_ID', 'ACC', ortholist)\n",
    "    converted_IDS = [v for k,v in converted.items()                # storing in a list all uniprot IDS \n",
    "                     if not 'ENS' in v]\n",
    "\n",
    "    manager = Manager()\n",
    "    appended_data = manager.list()\n",
    "    def getfeaturesparallel(k):\n",
    "        data = feat.getfeatures(k)                                 # using features class getfeatures function to retrieve a complete features dataset for each ID\n",
    "        appended_data.append(data)\n",
    "    Pool(threads).map(getfeaturesparallel, converted_IDS)          # activating getfeaturesparallel function in multiprocessing\n",
    "\n",
    "    allfeatures = pd.concat(appended_data)                         # concatenated dataframe with features infos\n",
    "\n",
    "    path = cwd + '/alignments/' + d + '/'\n",
    "    aln = json.load(open(path + d + '.json'))                      # opening the json format alignments containing file\n",
    "    alns = al.threshold_aln(\n",
    "        aln, alignment_threshold)                                  # set a threshold based on the alignment scores and return a dataframe\n",
    "    alns = feat.add_genes_ids(alns, converted)                     # add more infos in the dataframe\n",
    "    \n",
    "    features = feat.intersect_alns_features(alns, allfeatures)     # return an intersection between alignments dataframe and allfeatures dataframe\n",
    "    # --> feat.filter(allfeatures)\n",
    "    features.to_csv(cwd + '/features/' + d + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pair                                                                        112\n",
       "ab                                                                            B\n",
       "ensembl_ac_protein                                              ENSP00000284274\n",
       "ensembl_ac_protein_partner                                      ENSP00000274217\n",
       "ensembl_ac                                                      ENSG00000154124\n",
       "ensembl_ac_partner                                              ENSG00000145569\n",
       "uniprot_id                                                               Q96BN8\n",
       "uniprot_id_partner                                                       Q9NUU6\n",
       "gene id                                                                  OTULIN\n",
       "product                                           Ubiquitin thioesterase otulin\n",
       "begin                                                                       339\n",
       "end                                                                         339\n",
       "position                                                                    968\n",
       "scores                                                                  1.96824\n",
       "res                                                                           D\n",
       "res_partner                                                                   N\n",
       "ensembl_num                                                                 336\n",
       "ensembl_num_partner                                                         347\n",
       "type                                                                   ACT_SITE\n",
       "category                                                      DOMAINS_AND_SITES\n",
       "description                                                                 NaN\n",
       "alternativeSequence                                                         NaN\n",
       "EC number                                                                   NaN\n",
       "total_scores>1                                                          54.6465\n",
       "total_scores                                                           -24.8127\n",
       "col                           DDDDDD-DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD...\n",
       "col_partner                   NNNNN-NNNN-NNNNN-NNNDNNNNNNNNNNNNDNNNNNNNNNNNN...\n",
       "Name: 5788, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_table('/Users/carloderito/Desktop/TandemNeo/features/tandem.csv', sep=',')\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df[df['type'] == 'ACT_SITE'].drop_duplicates(['pair', 'ab']).loc[5788]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = cwd + '/alignments/' + d + '/'\n",
    "#x = json.load(open(path + d + '.json')) \n",
    "\n",
    "for aln in x.items():\n",
    "    if aln[0] == '27':\n",
    "        for p in x[aln[0]]['Positions'].items():\n",
    "            if p[1]['Score difference']>1:\n",
    "                for tag in p[1].items():\n",
    "                    #print(tag[0])\n",
    "                    if type(tag[1])== dict:\n",
    "                        for ac in p[1][tag[0]].keys():\n",
    "                            #if ac == 'ENSP00000226279':\n",
    "                                #if type(ac[1])== dict:\n",
    "                                #print(aln[0],p[0],tag[0],ac[0],ac[1]['Positions'],ac[1]['Residue'],tag[1]['Column'],p[1]['Score difference'],)\n",
    "                            print(tag, p[1]['Score difference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3.7",
   "language": "python",
   "name": "myenv3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
