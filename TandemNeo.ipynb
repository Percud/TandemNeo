{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /usr/bin/env python\n",
    "\n",
    "import requests, sys, os, pandas as pd, numpy as np, re, gzip, itertools, json, csv, shutil, xmltodict, urllib.request\n",
    "from Bio.Blast.Applications import NcbiblastpCommandline as blastp\n",
    "from Bio.Blast.Applications import NcbimakeblastdbCommandline as makeblastdb\n",
    "from multiprocessing import Pool\n",
    "from ftplib import FTP\n",
    "from Bio import SeqIO\n",
    "import natsort as ns\n",
    "from natsort import natsorted\n",
    "from Bio import Align, AlignIO, SeqIO, Phylo\n",
    "from Bio.Align.Applications import ClustalOmegaCommandline as clustalo\n",
    "from Bio.Align import substitution_matrices, MultipleSeqAlignment as MSA, AlignInfo\n",
    "from Bio.PDB import PDBParser as PDB, PDBIO\n",
    "from itertools import combinations\n",
    "from random import sample\n",
    "import statistics as st\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SPECIES SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING DIRECTORIES\n",
    "\n",
    "if not os.path.exists(cwd + '/fa') and not os.path.exists(cwd + '/gtf') and not os.path.exists(cwd + '/main') and not os.path.exists(cwd + '/blast_queries') and not os.path.exists(cwd + '/tandem') and not os.path.exists(cwd + '/orthologues') and not os.path.exists(cwd + '/appris') and not os.path.exists(cwd + '/alignments') and not os.path.exists(cwd + '/uniprot'):\n",
    "    os.mkdir(cwd + '/fa'), os.mkdir(cwd + '/gtf'), os.mkdir(cwd + '/main'), os.mkdir(cwd + '/blast_queries'), os.mkdir(cwd + '/tandem'), os.mkdir(cwd + '/appris'), os.mkdir(cwd + '/orthologues'), os.mkdir(cwd + '/alignments'), os.mkdir(cwd + '/uniprot')\n",
    "\n",
    "# SPECIES LIST\n",
    "\n",
    "assemblies = requests.get(\"http://rest.ensembl.org/info/species?\", headers={\"Content-Type\": \"application/json\"}).json()\n",
    "json.dump(assemblies, open('assemblies.txt', 'w'))\n",
    "\n",
    "def getaxonomy(specie):\n",
    "    \"\"\"return class, order, taxid, publications for a give specie name\"\"\"\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        eutils = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/'\n",
    "        taxid = xmltodict.parse(requests.get(eutils + 'esearch.fcgi?db=taxonomy&term=' + specie).content)['eSearchResult']['IdList']['Id']\n",
    "        pubs = xmltodict.parse(requests.get(eutils + 'esearch.fcgi?db=pubmed&term=' + specie).content)['eSearchResult']['Count']\n",
    "        taxons = xmltodict.parse(requests.get(eutils + 'efetch.fcgi?db=taxonomy&id=' + taxid + '&retmode=xml&rettype=full').content)['TaxaSet']['Taxon']['LineageEx']['Taxon']\n",
    "\n",
    "        scientific_name = [taxons[x]['ScientificName'] for x in range(len(taxons)) if taxons[x]['ScientificName'] in ['Sauropsida', 'Mammalia', 'Actinopteri']]\n",
    "        orders = [taxons[x]['ScientificName'] for x in range(len(taxons)) if taxons[x]['Rank'] == 'order']\n",
    "        \n",
    "        return scientific_name + orders + [specie.split('_', 1)[0]] + [specie.split('_', 1)[1]] + [taxid] + [pubs]\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "available_species = []\n",
    "for assembly in assemblies['species']:\n",
    "    taxonomy = getaxonomy(assembly['name'].capitalize())\n",
    "    if not taxonomy == None and len(taxonomy) == 6:\n",
    "        available_species.append(taxonomy)\n",
    "        \n",
    "\n",
    "species_list_df = pd.DataFrame(available_species)\n",
    "species_list_df[5] = species_list_df[5].astype(int)\n",
    "species_list_df_uniquegenre = species_list_df.sort_values(5, ascending=False).groupby(2, sort=False).head(1)\n",
    "species_list_df_cap = species_list_df_uniquegenre.groupby(1, sort=False).head(12).sort_values([0, 1])\n",
    "\n",
    "for line in species_list_df_cap.values.tolist():\n",
    "    print(*line, sep=' ', file=open('species_list.txt', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DOWNLOAD .faa AND .gtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD\n",
    "\n",
    "species = [line.split()[2] + '_' + line.split()[3] for line in open('species_list.txt')]\n",
    "queries = ['Homo_sapiens', 'Gallus_gallus', 'Danio_rerio']\n",
    "\n",
    "assemblies = requests.get(\"http://rest.ensembl.org/info/species?\", headers={\"Content-Type\": \"application/json\"}).json()\n",
    "release = [line['release'] for line in assemblies['species'] if line['name'] == 'homo_sapiens']\n",
    "\n",
    "def getassembly(specie):\n",
    "    \"\"\"return assembly identifier for a given species name\"\"\"\n",
    "    \n",
    "    for assembly in assemblies['species']:\n",
    "        if specie.lower() == assembly['name']:\n",
    "            return assembly['assembly']\n",
    "        \n",
    "def getassembly(specie):\n",
    "    \"\"\"return assembly identifier for a given species name\"\"\"\n",
    "    \n",
    "    for assembly in assemblies['species']:\n",
    "        if specie.lower() == assembly['name']:\n",
    "            return assembly['assembly']\n",
    "        \n",
    "def download(specie):\n",
    "    \"\"\"download faa from ensembl ftp\"\"\"\n",
    "    \n",
    "    ftp = FTP('ftp.ensembl.org')\n",
    "    ftp.login()\n",
    "    ftp.cwd('/pub/current_fasta/' + specie.lower() + '/pep/')\n",
    "    ftp.retrbinary('RETR ' + specie + '.' + getassembly(specie) + '.pep.all.fa.gz', open(cwd + '/fa/' + specie + '.fa.gz', 'wb').write)\n",
    "    if specie in queries:\n",
    "        ftp.cwd('/pub/current_gtf/' + specie.lower() + '/')\n",
    "        ftp.retrbinary(\"RETR \" + specie + '.' + getassembly(specie) + '.' + str(release[0]) + '.gtf.gz', open(cwd + '/gtf/' + specie + '.gtf.gz', 'wb').write)\n",
    "    ftp.quit()\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(processes=20)\n",
    "    pool.map(download, [specie for specie in species])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MAIN ISOFORMS AND BLASTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN ISOFORMS AND BLASTP\n",
    "\n",
    "species = [line.split()[2] + '_' + line.split()[3] for line in open('species_list.txt')]\n",
    "queries = ['Homo_sapiens', 'Gallus_gallus', 'Danio_rerio']\n",
    "\n",
    "assemblies = requests.get(\"http://rest.ensembl.org/info/species?\", headers={\"Content-Type\": \"application/json\"}).json()\n",
    "\n",
    "def getassembly(specie):\n",
    "    \"\"\"return assembly identifier for a given species name\"\"\"\n",
    "    \n",
    "    for assembly in assemblies['species']:\n",
    "        if specie.lower() == assembly['name']:\n",
    "            return assembly['assembly']\n",
    "\n",
    "for line in queries:\n",
    "    \n",
    "    appris = pd.read_table(urllib.request.urlopen(\"http://apprisws.bioinfo.cnio.es/pub/current_release/datafiles/\" + line.lower() + \"/\" + getassembly(line.lower()) + \"/appris_data.principal.txt\"), header=None)\n",
    "    appris = appris[appris[4].str.contains('PRINCIPAL')].sort_values([1, 4]).drop_duplicates(1)\n",
    "    for line2 in appris.values.tolist():\n",
    "        print(*line2, sep='\\t', file=open(cwd + '/appris/' + line + '_principal_appris.txt', 'a'))\n",
    "    main_isoforms = appris[2].values.tolist()\n",
    "\n",
    "    for fasta in SeqIO.parse(gzip.open(cwd + '/fa/' + line + '.fa.gz', 'rt'), 'fasta'):\n",
    "        if 'gene_biotype:protein_coding' in fasta.description and re.split('\\s', fasta.description)[4].split('transcript:')[1].split('.')[0] in main_isoforms:\n",
    "            print('>' + fasta.id + '\\n' + fasta.seq, file=open(cwd + '/main/' + line + '_main.fa', 'a'))\n",
    "\n",
    "    gtf = pd.read_table(cwd + '/gtf/' + line + '.gtf.gz', compression='gzip', comment='#', header=None)\n",
    "    gtf = gtf.join(gtf[8].str.split('\"', expand=True).add_prefix('sec'))           \n",
    "    gtf = gtf[gtf['sec5'].isin(main_isoforms)]\n",
    "\n",
    "    main = []\n",
    "    for line3 in gtf.values.tolist():\n",
    "        if 'CDS' in line3[2]:\n",
    "            chromosome, strand, gene_id, transcript_id, gene_name = line3[0], line3[6], line3[10], line3[14], line3[20]\n",
    "            protein_id = [line for line in line3 if str(line).startswith(gene_id.split('0')[0][:-1] + 'P')]\n",
    "        elif 'start' in line3[2]:\n",
    "            start = line3[3]\n",
    "        elif 'stop' in line3[2]:\n",
    "            stop = line3[4]\n",
    "            main.append([chromosome, gene_id, strand, protein_id[0], start, stop, transcript_id, gene_name])\n",
    "\n",
    "    tsv = pd.DataFrame(main, columns=['Chromosome', 'Gene_id', 'Strand', 'Protein_id', 'Start', 'Stop', 'Transcript_id', 'Gene_name'])\n",
    "    tsv['Chromosome'] = pd.Categorical(tsv['Chromosome'], ordered=True, categories=ns.natsorted(tsv['Chromosome'].unique()))\n",
    "    tsv['Mean'] = tsv[['Start', 'Stop']].mean(axis=1)\n",
    "    tsv = tsv.sort_values(['Chromosome', 'Mean'], ascending=(True, True)).drop(columns='Mean').to_csv((cwd + '/main/' + line + '.tsv'), index=False, sep='\\t')\n",
    "    \n",
    "    makeblastdb(dbtype='prot', input_file=(cwd + '/main/' + line + '_main.fa'))()\n",
    "    blastp(query=(cwd + '/main/' + line + '_main.fa'), db=(cwd + '/main/' + line + '_main.fa'), num_threads=20, evalue='10e-6', max_hsps=1, outfmt=7, max_target_seqs=5, out=cwd + '/blast_queries/' + line + '_blast.txt')()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DUPLICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUPLICATIONS\n",
    "\n",
    "for file in os.listdir(cwd + '/blast_queries'):\n",
    "    if file.endswith('blast.txt'):\n",
    "\n",
    "        x = pd.read_table(cwd + '/blast_queries/' + file, comment='#', header=None, names=['Query', 'Subject', 'Identity', 'A.lenght', 'Mismatches', 'Gap', 'Q.start', 'Q.end', 'S.start', 'S.end', 'Evalue', 'Bits'])\n",
    "        x = x[x['Identity'] != 100].drop_duplicates('Query')\n",
    "        x['Query'] = x['Query'].apply(lambda x: x.split('.')[0])\n",
    "        x['Subject'] = x['Subject'].apply(lambda x: x.split('.')[0])\n",
    "        columnab = [[line[0], line[1]] for line in x.values.tolist()]\n",
    "        columnabbrh = [line for line in [[line[0], line[1]] for line in x.values.tolist()] if [line[1], line[0]] in columnab]\n",
    "        xbrhdf = pd.merge(x, pd.DataFrame(columnabbrh), left_on=['Query', 'Subject'], right_on=[0, 1]).drop(columns=0)\n",
    "        xy = pd.merge(pd.read_table(cwd + '/main/' + file.split('_blast')[0] + '.tsv'), xbrhdf, right_on='Query', left_on='Protein_id', how='outer').drop(columns=[1, 'Query'])\n",
    "        xy = xy.values.tolist()\n",
    "\n",
    "        tandem, convergent, divergent = [], [], []\n",
    "        for x in range(len(xy)-1):\n",
    "            strand, start, stop = xy[x][2], xy[x][4], xy[x][5]\n",
    "            strand2, start2, stop2 = xy[x+1][2], xy[x+1][4], xy[x+1][5]\n",
    "            query, subject = xy[x][3], xy[x+1][8]\n",
    "            if query == subject:\n",
    "                if strand == strand2:\n",
    "                    if (strand == '+' and stop < start2) or (strand == '-' and start < stop2):\n",
    "                        tandem.append([xy[x][1], xy[x][3], xy[x+1][1], xy[x+1][3], xy[x][9], xy[x][17]])\n",
    "                if strand != strand2:\n",
    "                    if strand == '+' and stop < stop2:\n",
    "                        convergent.append([xy[x][1], xy[x][3], xy[x+1][1], xy[x+1][3], xy[x][9], xy[x][17]])\n",
    "                    if strand == '-' and start < start2:\n",
    "                        divergent.append([xy[x][1], xy[x][3], xy[x+1][1], xy[x+1][3], xy[x][9], xy[x][17]])\n",
    "                        \n",
    "        for line in tandem:\n",
    "            print(*line, sep='\\t', file=open(cwd + '/tandem/' + file.split('_blast')[0] + '_tandem.tsv', 'a'))\n",
    "        for line in divergent:\n",
    "            print(*line, sep='\\t', file=open(cwd + '/tandem/' + file.split('_blast')[0] + '_divergent.tsv', 'a'))\n",
    "        for line in convergent:\n",
    "            print(*line, sep='\\t', file=open(cwd + '/tandem/' + file.split('_blast')[0] + '_convergent.tsv', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ORTHOLOGUES (COMPARA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARA ORTHOLOGUES\n",
    "\n",
    "def getorthoss(line):\n",
    "    \"\"\"return a column of orthologues for a given query\"\"\"\n",
    "    \n",
    "    def getorthoss_letter(query, count, letter, specie, types):\n",
    "\n",
    "        try:\n",
    "\n",
    "            r = requests.get(\"https://rest.ensembl.org/homology/id/\" + query + \"?type=orthologues;aligned=0;cigar_line=0;compara=vertebrates\", headers={\"Content-Type\": \"application/json\"})\n",
    "            ortho = r.json()\n",
    "\n",
    "            for x in range(len(ortho['data'][0]['homologies'])):\n",
    "                target = ortho['data'][0]['homologies'][x]['target']\n",
    "                per, pid, gid, spe = target['perc_id'], target['protein_id'], target['id'], target['species']\n",
    "                source = ortho['data'][0]['homologies'][x]['source']\n",
    "                per2, pid2, gid2, spe2 = source['perc_id'], source['protein_id'], source['id'], source['species']\n",
    "                print(pid, gid, spe.capitalize(), count, letter, per, specie, str(count) + letter, file=open(cwd + '/orthologues/' + specie + '_' + types + '_orthologues.txt', 'a'))\n",
    "            print(pid2, gid2, spe2.capitalize(), count, letter, 100, specie, str(count) + letter, file=open(cwd + '/orthologues/' + specie + '_' + types + '_orthologues.txt', 'a'))\n",
    "\n",
    "        except:\n",
    "            print(query, 'no_data', 'no_data', count, letter, 0, specie, str(count) + letter, file=open(cwd + '/orthologues/' + specie + '_' + types + '_orthologues.txt', 'a'))\n",
    "    \n",
    "    getorthoss_letter(line.split()[0], counter.get(line.split()[0]), 'A', line.split()[6].capitalize(), line.split()[7])\n",
    "    getorthoss_letter(line.split()[2], counter.get(line.split()[2]), 'B', line.split()[6].capitalize(), line.split()[7])\n",
    "       \n",
    "        \n",
    "counter = {} # assign a count for each pairs based on genomes\n",
    "for file in os.listdir(cwd + '/tandem'):\n",
    "    species = file.split('_')[0] + '_' + file.split('_')[1]\n",
    "    types = file.split('_')[2].split('.')[0]\n",
    "        \n",
    "    for n, line in enumerate(open(cwd + '/tandem/' + file)):\n",
    "        counter.update({line.split()[0]: n}), counter.update({line.split()[2]: n})\n",
    "        \n",
    "    #for line in open(cwd + '/tandem/' + file):\n",
    "    #    getorthoss(line + '\\t' + species + '\\t' + types)\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        pool = Pool(processes=4)\n",
    "        pool.map(getorthoss, [line + '\\t' + species + '\\t' + types for line in open(cwd + '/tandem/' + file)])\n",
    "\n",
    "def orthologues_df(kind):\n",
    "\n",
    "    # creo un dizionario con la tassonomia a partire dalla lista delle specie (specie, ordine, classe)\n",
    "    classes, orders = {}, {}\n",
    "    for line in open('species_list.txt'):\n",
    "        classes.update({line.split()[2] + '_' + line.split()[3]: line.split()[0]})\n",
    "        orders.update({line.split()[2] + '_' + line.split()[3]: line.split()[1]})\n",
    "\n",
    "    # apro le tre tabelle\n",
    "    h = pd.read_table(cwd + '/orthologues/Homo_sapiens_' + kind + '_orthologues.txt', sep='\\s', engine='python', header=None)\n",
    "    d = pd.read_table(cwd + '/orthologues/Danio_rerio_' + kind + '_orthologues.txt', sep='\\s', engine='python', header=None)\n",
    "    g = pd.read_table(cwd + '/orthologues/Gallus_gallus_' + kind + '_orthologues.txt', sep='\\s', engine='python', header=None)\n",
    "\n",
    "    # aggiungo lettere corrispondenti alle specie queries\n",
    "    h[7] = 'H.' + h[7]\n",
    "    d[7] = 'D.' + d[7]\n",
    "    g[7] = 'G.' + g[7]\n",
    "\n",
    "    # concateno le tre tabelle una dietro l'altra\n",
    "    nobrh = pd.concat([h,d,g])\n",
    "\n",
    "    # aggiungo voci della tassonomia\n",
    "    nobrh[8] = nobrh[2].apply(lambda x: classes.get(x))\n",
    "    nobrh[9] = nobrh[2].apply(lambda x: orders.get(x))\n",
    "    nobrh = nobrh[(nobrh[8] == 'Mammalia') | (nobrh[8] == 'Sauropsida') | (nobrh[8] == 'Actinopteri')]\n",
    "    nobrh[10] = nobrh[7].apply(lambda x: re.split('\\.', x)[0])\n",
    "    nobrh = nobrh.dropna(subset=[8])\n",
    "\n",
    "    # numeriamo le hit di compara per ogni query per ogni specie (one-to-many)\n",
    "    # se con gene di gallus da 20 ortologhi, numera questi ortologhi da 1 a 20\n",
    "    nobrhsort = nobrh.sort_values([10, 3, 4, 2, 5], ascending=(False, True, True, True, False))\n",
    "    nobrhsort[11] = nobrhsort.groupby([2, 7]).cumcount()+1\n",
    "\n",
    "    # drop duplicati stessa percentuale d'identità per ogni query per ogni specie per il sorting\n",
    "    nobrhsort = nobrhsort.drop_duplicates([2, 5, 7])\n",
    "\n",
    "    # sorting per percentuale d'identità globale, drop geni duplicati tenendo il primo (one-to-one), drop stessa specie stesso gruppo \n",
    "    # \"best-hit\"\n",
    "    bh = nobrhsort.sort_values(5, ascending=False).drop_duplicates(1).drop_duplicates([2,7])\n",
    "\n",
    "    # tiene solo le prime hit di one-to-one\n",
    "    # come nei casi in cui da ortologo come prima hit quando in realtà era il terzo\n",
    "    # \"reciprocal\"\n",
    "    brh = bh[bh[11] == 1]\n",
    "    brh = brh.sort_values([10, 3, 4], ascending=(False, True, True))\n",
    "    brh[20] = brh.groupby(7, sort=False)[7].transform(\"count\")\n",
    "\n",
    "    counts = brh[[7, 20]].drop_duplicates(7).values.tolist()\n",
    "    couple_high_then3 = []\n",
    "    for x in range(len(counts)-1):\n",
    "        if counts[x][1] and counts[x+1][1] > 3 and re.split('A|B', counts[x][0])[0].split('.')[1] == re.split('A|B', counts[x+1][0])[0].split('.')[1]:\n",
    "            couple_high_then3.append(counts[x][0])\n",
    "            couple_high_then3.append(counts[x+1][0])\n",
    "    brhfull = brh[brh[7].isin(couple_high_then3)]\n",
    "    brhfull = brhfull.reset_index().drop(columns=['index'])\n",
    "    brhfull[10] = brhfull.groupby([3,6], sort=False, as_index=False).ngroup()\n",
    "    brhfull[10] = brhfull[10].astype(str) + brhfull[4]\n",
    "\n",
    "    # pivot table\n",
    "    brhfullpivot = brhfull.pivot(index=2, columns=10, values=0)\n",
    "    brhfullpivot = brhfullpivot.reset_index()\n",
    "    brhfullpivot['Classes'] = brhfullpivot[2].apply(lambda x: classes.get(x))\n",
    "    brhfullpivot['Orders'] = brhfullpivot[2].apply(lambda x: orders.get(x))\n",
    "    brhfullpivot = brhfullpivot.rename(columns={2: 'Species'})\n",
    "    brhfullpivot = brhfullpivot.set_index(['Classes', 'Orders', 'Species']).sort_index()\n",
    "    brhfullpivot = brhfullpivot.reindex(natsorted(brhfullpivot.columns), axis=1)\n",
    "    brhfullpivot.to_csv(cwd + '/orthologues/' + kind + '_orthologues.csv', sep=';')\n",
    "    \n",
    "orthologues_df('tandem')\n",
    "orthologues_df('divergent')\n",
    "orthologues_df('convergent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATABASE\n",
    "\n",
    "tandem_orthologs = pd.read_table(cwd + '/orthologues/' + 'tandem_orthologues.csv', sep=';').set_index(['Classes', 'Orders'])\n",
    "convergent_orthologs = pd.read_table(cwd + '/orthologues/' + 'convergent_orthologues.csv', sep=';').set_index(['Classes', 'Orders'])\n",
    "divergent_orthologs = pd.read_table(cwd + '/orthologues/' + 'divergent_orthologues.csv', sep=';').set_index(['Classes', 'Orders'])\n",
    "orthologs = tandem_orthologs.values.tolist() + divergent_orthologs.values.tolist() + convergent_orthologs.values.tolist()\n",
    "\n",
    "data = {}\n",
    "for x in range(len(orthologs)):\n",
    "    specie = orthologs[x][0] \n",
    "    \n",
    "    main = {}\n",
    "    for fasta in SeqIO.parse(gzip.open(cwd + '/fa/' + specie + '.fa.gz', 'rt'), 'fasta'):\n",
    "\n",
    "        splitted = re.split('\\s', fasta.description)\n",
    "        pid = splitted[0].split('.')[0]\n",
    "        gid = splitted[3].split('gene:')[1].split('.')[0]\n",
    "        tra = splitted[4].split('transcript:')[1].split('.')[0]\n",
    "        chrom = splitted[2].split(':')[2]\n",
    "        strand = splitted[2].split(':')[5]\n",
    "        if strand == '1':\n",
    "            sta, sto = splitted[2].split(':')[3], splitted[2].split(':')[4]\n",
    "        elif strand == '-1':\n",
    "            sta, sto = splitted[2].split(':')[4], splitted[2].split(':')[3]\n",
    "        try:\n",
    "            symbol = splitted[7].split('gene_symbol:')[1]\n",
    "        except:\n",
    "            symbol = 'no_data'\n",
    "        try:\n",
    "            if '[Source' in fasta.description:\n",
    "                product = re.split('description:', fasta.description)[1].split(' [Source')[0]\n",
    "            else:\n",
    "                product = re.split('description:', fasta.description)[1]\n",
    "        except:\n",
    "            product = 'no_data'\n",
    "\n",
    "        main.update({pid: [str(fasta.seq), gid, chrom, strand, int(sta), int(sto), len(fasta.seq), product.strip(), symbol, specie]})\n",
    "\n",
    "    for line in orthologs[x]:\n",
    "        if not 'nan' in str(line):\n",
    "            data.update({line: main.get(line)})\n",
    "                \n",
    "json.dump(data, open('database.json', 'w')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. FASTAS FOR ALIGNMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTA FOR ALIGNMENTS\n",
    "\n",
    "database = json.load(open('database.json'))\n",
    "\n",
    "if not os.path.exists(cwd + '/alignments/tandem_alignments') and not os.path.exists(cwd + '/alignments/convergent_alignments') and not os.path.exists(cwd + '/alignments/divergent_alignments'):\n",
    "    os.mkdir(cwd + '/alignments/tandem_alignments'), os.mkdir(cwd + '/alignments/convergent_alignments'), os.mkdir(cwd + '/alignments/divergent_alignments')\n",
    "    \n",
    "def fastaforaligments(kind):\n",
    "    data = pd.read_csv(cwd + '/orthologues/' + kind + '_orthologues.csv', sep=';').set_index(['Classes', 'Orders', 'Species'])\n",
    "    pairs_ab = list(zip([line for line in data.columns.tolist() if 'A' in line], [line for line in data.columns.tolist() if 'B' in line]))\n",
    "\n",
    "    for x in range(len(pairs_ab)):\n",
    "        for f in data[list(pairs_ab[x])].values.tolist():\n",
    "\n",
    "            def getfastas(symbol, product, lenght, gene, specie, tandem, pid, fasta):\n",
    "                    print('>' + pid + '\\t' + gene + '\\t' + lenght + '\\t' + symbol + '\\t' + product + '\\t' + specie + '\\t' + tandem + '\\n' + fasta, file=open(cwd + '/alignments/' + kind + '_alignments' + '/' + re.split('A|B', tandem)[0] + '.fa', 'a'))            \n",
    "\n",
    "            if not 'nan' in str(f[0]): \n",
    "                getfastas(database.get(f[0])[8], database.get(f[0])[7], \n",
    "                          str(len(database.get(f[0])[0])), database.get(f[0])[1], \n",
    "                          database.get(f[0])[9], data[list(pairs_ab[x])].columns.tolist()[0], \n",
    "                          f[0], database.get(f[0])[0])\n",
    "                \n",
    "            if not 'nan' in str(f[1]):\n",
    "                getfastas(database.get(f[1])[8], database.get(f[1])[7], \n",
    "                          str(len(database.get(f[1])[0])), database.get(f[1])[1], \n",
    "                          database.get(f[1])[9], data[list(pairs_ab[x])].columns.tolist()[1], \n",
    "                          f[1], database.get(f[1])[0])\n",
    "            \n",
    "fastaforaligments('tandem')\n",
    "fastaforaligments('convergent')\n",
    "fastaforaligments('divergent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. ALIGNMENTS (CLUSTALO + SCORING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALIGNMENTS\n",
    "\n",
    "def conservation(msa,matrix): \n",
    "    aligner = Align.PairwiseAligner()\n",
    "    aligner.substitution_matrix = substitution_matrices.load(matrix)\n",
    "    numbers, columns, scores = [], [], []\n",
    "    for n in range(msa.get_alignment_length()):\n",
    "        columns.append((msa[:,n]))\n",
    "        numbers.append(n+1)\n",
    "    for c in columns:\n",
    "        c = list(c)\n",
    "        if 'X' in c:\n",
    "            c.remove('X')\n",
    "        pairs = list(combinations(c,2))\n",
    "        score = []\n",
    "        try:\n",
    "            for p in pairs:\n",
    "                if not '-' in p:\n",
    "                    score.append(aligner.score(p[0],p[1]))\n",
    "            scores.append(sum(score)/len(pairs))\n",
    "        except:\n",
    "            pass\n",
    "    return (list(zip(numbers,columns,scores)))\n",
    "\n",
    "def getalignmentjson(folder):\n",
    "    \"\"\"return similarity, number of sequences, alignment lenght, position etc.. for a given aligned fasta file\"\"\"\n",
    "\n",
    "    alignments_log = {}\n",
    "\n",
    "    # define filename    \n",
    "    number_of_files = max([int(file.split('.')[0]) for file in os.listdir(cwd + '/alignments/' + folder) if file.endswith('.fa')])\n",
    "    for name in range(number_of_files):\n",
    "        name = str(name)\n",
    "        fa = cwd + '/alignments/' + folder + '/' + name + '.fa'\n",
    "        fasta = cwd + '/alignments/' + folder + '/' +  name + '.fasta'\n",
    "        matrix = 'BLOSUM62'\n",
    "\n",
    "        len_A, len_B = [], []\n",
    "        for f in list(SeqIO.parse(fa, 'fasta')):\n",
    "            if 'A' in f.description.split('\\t')[-1]:\n",
    "                len_A.append(int(f.description.split('\\t')[2]))\n",
    "            elif 'B' in f.description.split('\\t')[-1]:        \n",
    "                len_B.append(int(f.description.split('\\t')[2]))\n",
    "\n",
    "        similarity = round(min(st.mean(len_A),st.mean(len_B))/max(st.mean(len_A),st.mean(len_B))*100,2)\n",
    "        if similarity > 60:\n",
    "\n",
    "            clustalo(infile = fa, outfile = fasta, force = False)()\n",
    "            # sampling sequences from alignments\n",
    "            alignment = AlignIO.read(fasta, 'fasta')\n",
    "\n",
    "            dif_score, A, B, gap = [], [], [], []\n",
    "            for a in alignment:\n",
    "                if 'A' in a.description.split('\\t')[-1]:\n",
    "                    A.append(a) \n",
    "                elif 'B' in a.description.split('\\t')[-1]:\n",
    "                    B.append(a)\n",
    "            A, B = sample(A, min(len(A),len(B))), sample(B, min(len(A),len(B)))\n",
    "            aln_A, aln_B, aln = MSA(A), MSA(B), MSA(A+B)\n",
    "            cons_A, cons_B = AlignInfo.SummaryInfo(aln_A).dumb_consensus(), AlignInfo.SummaryInfo(aln_B).dumb_consensus()\n",
    "\n",
    "            differences = list(zip(conservation(aln,matrix), conservation(aln_A,matrix), conservation(aln_B,matrix)))   \n",
    "            hit, res_A, res_B = [], [], []\n",
    "            for d in differences:\n",
    "                dif_score.append([d[0][0], d[1][1], d[2][1], round(min(d[1][2], d[2][2])-d[0][2],2)])\n",
    "                hit.append(d[0][0])\n",
    "                res_A.append(cons_A[d[0][0]-1])\n",
    "                res_B.append(cons_B[d[0][0]-1])\n",
    "\n",
    "            pos_A, pos_B, pos_C = [], [], []\n",
    "            for n in range(len(alignment)):\n",
    "                for res in zip(res_A,hit,res_B):\n",
    "                    if 'ENSP0' in alignment[n].name:\n",
    "                        try:\n",
    "                            pos_C.append([res[0], res[2]])\n",
    "                            if 'B' in alignment[n].description.split('\\t')[-1][-1]:\n",
    "                                pos_B.append(res[1]-list(alignment[n][0:res[1]-1]).count('-'))\n",
    "                            else:\n",
    "                                pos_B.append(None)\n",
    "                            if 'A' in alignment[n].description.split('\\t')[-1][-1]:\n",
    "                                pos_A.append(res[1]-list(alignment[n][0:res[1]-1]).count('-'))\n",
    "                            else:\n",
    "                                pos_A.append(None)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "            x = pd.concat([pd.Series(pos_A).dropna().reset_index().astype(np.int64), pd.Series(pos_B).dropna().reset_index().astype(np.int64)], axis=1).drop(columns='index').astype(str)\n",
    "            y = zip(x.values.tolist(), pos_C)\n",
    "            ratio = round(len(hit)/np.mean(min(len_A,len_B))*100, 2)\n",
    "\n",
    "            positions = {}\n",
    "            for p in list(zip(dif_score, y)):\n",
    "                key = p[0][0]\n",
    "                alignment_A = p[0][1]\n",
    "                alignment_B = p[0][2]\n",
    "                score = p[0][3]\n",
    "                homo_position_A = p[1][0][0]\n",
    "                homo_position_B = p[1][0][1]\n",
    "                homo_residue_A = p[1][1][0]\n",
    "                homo_residue_B = p[1][1][1]\n",
    "                position = {'alignment_A': alignment_A, 'alignment_B': alignment_B, 'Score': score, \n",
    "                            'Homo_position_A': homo_position_A, 'Homo_position_B': homo_position_A, \n",
    "                            'Homo_residue_A': homo_residue_A, 'homo_residue_B': homo_residue_B}\n",
    "                positions.update({key: position})\n",
    "\n",
    "            accessions = []\n",
    "            if any('ENSP0'  in string for string in [fasta.id for fasta in SeqIO.parse\n",
    "                                                     (cwd + '/alignments/' + folder + '/' + name + '.fa', 'fasta')\n",
    "                                                     if 'A' in fasta.description.split('\\t')[6]]) == False:\n",
    "                accessions.append(None)\n",
    "            if any('ENSP0'  in string for string in [fasta.id for fasta in SeqIO.parse\n",
    "                                                     (cwd + '/alignments/' + folder + '/' + name + '.fa', 'fasta')\n",
    "                                                     if 'B' in fasta.description.split('\\t')[6]]) == False:\n",
    "                accessions.append(None)\n",
    "            for fasta in SeqIO.parse(cwd + '/alignments/' + folder + '/' + name + '.fa', 'fasta'):\n",
    "                if 'ENSP0' in fasta.id and 'A' in fasta.description.split('\\t')[6]:\n",
    "                    accessions.append(fasta.id) \n",
    "                if 'ENSP0' in fasta.id and 'B' in fasta.description.split('\\t')[6]:\n",
    "                    accessions.append(fasta.id) \n",
    "\n",
    "\n",
    "            alignments_log.update({name: {'Accessions': accessions, 'Similarity': similarity, 'Number_of_sequences': len(list(SeqIO.parse(fa, 'fasta'))), 'Alignment_lenght': aln.get_alignment_length(), 'Ratio': ratio, 'Positions': positions, 'Scores': [v['Score'] for k,v in positions.items()]}})\n",
    "\n",
    "\n",
    "    json.dump(alignments_log, open(cwd + '/alignments/' + folder + '/alignments.json', 'w'))\n",
    "    \n",
    "\n",
    "getalignmentjson('tandem_alignments')\n",
    "getalignmentjson('convergent_alignments')\n",
    "getalignmentjson('divergent_alignments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. FEATURES (UNIPROT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(kind):\n",
    "\n",
    "    # apro tabella ortologhi e creo lista degli accession di Homo da analizzare\n",
    "    orthos = pd.read_table(cwd + '/orthologues/' + kind + '_orthologues.csv', sep=';').set_index(['Classes', 'Orders', 'Species'])\n",
    "    homo = pd.DataFrame(orthos.loc[('Mammalia', 'Primates', 'Homo_sapiens')])\n",
    "    homo_joined = ' '.join(map(str, [line[0] for line in homo.values.tolist()]))\n",
    "\n",
    "    # preparo gli accession per la conversione Ensembl_ID --> Uniprot_ID\n",
    "    params = {'from': 'ENSEMBL_PRO_ID', 'to': 'ACC', 'format': 'tab', 'query': homo_joined}\n",
    "    url = 'https://www.uniprot.org/uploadlists/'\n",
    "    req = urllib.request.Request(url, urllib.parse.urlencode(params).encode('utf-8'))\n",
    "    with urllib.request.urlopen(req) as f:\n",
    "        response = f.read()\n",
    "\n",
    "    # scrivo un dizionario di conversione\n",
    "    converted = {}\n",
    "    for line in response.decode('utf-8').split('\\n'):\n",
    "        if not 'From' in line and not line.split() == []:\n",
    "            converted.update({line.split()[1]: line.split()[0]})\n",
    "\n",
    "    # recupero le features da uniprot in formato json\n",
    "    def getfeatures(uniprot_id):\n",
    "        \"\"\"return the Uniprot complete dataset for a given Uniprot ID\"\"\"\n",
    "        try:\n",
    "            r = requests.get(\"https://www.ebi.ac.uk/proteins/api/proteins?size=-1&accession=\" + uniprot_id, headers={ \"Accept\" : \"application/json\"})\n",
    "            data = pd.json_normalize(r.json())\n",
    "            return data\n",
    "        except:\n",
    "            return str(uniprot_id) + ' not_found'\n",
    "\n",
    "    # concateno le features in formato tabulare e le scarico in locale\n",
    "    appended_data = []\n",
    "    for k, v in converted.items():\n",
    "        data = getfeatures(k)\n",
    "        appended_data.append(data)\n",
    "    appended_data = pd.concat(appended_data)\n",
    "    appended_data.to_csv(cwd + '/uniprot/' + kind + '_raw_features.csv')\n",
    "\n",
    "    raw_features = pd.read_table(cwd + '/uniprot/' + kind + '_raw_features.csv', sep=',')\n",
    "    raw_features = raw_features[raw_features['accession'].isin(list(converted.keys()))]\n",
    "\n",
    "    # organizzo e filtro le features, unendole in un unico dataframe\n",
    "    dataframe_list = []\n",
    "    for line in raw_features[['accession', 'gene', 'protein.recommendedName.ecNumber', 'protein.recommendedName.fullName.value', 'features']].iterrows():\n",
    "        if not str(line[1]['features']) == 'nan' and 'description' in list(eval(line[1]['features'])[0].keys()):\n",
    "            dataframe = pd.DataFrame(eval(line[1]['features']))[['type', 'category', 'description', 'begin', 'end']]\n",
    "            dataframe['accession'] = line[1]['accession']\n",
    "            if not str(line[1]['gene']) == 'nan':\n",
    "                dataframe['gene'] = eval(line[1]['gene'])[0]['name']['value']\n",
    "            else:\n",
    "                dataframe['gene'] = 'NaN'\n",
    "            if not str(line[1]['protein.recommendedName.ecNumber']) == 'nan':\n",
    "                dataframe['EC_number'] = eval(line[1]['protein.recommendedName.ecNumber'])[0]['value']\n",
    "            else:\n",
    "                dataframe['EC_number'] = 'NaN'\n",
    "            dataframe['full_protein_name'] = line[1]['protein.recommendedName.fullName.value']\n",
    "            dataframe['ensembl_accession'] = converted.get(line[1]['accession'])\n",
    "            dataframe_list.append(dataframe)\n",
    "\n",
    "    df = pd.concat(dataframe_list)[['accession', 'ensembl_accession', 'gene', 'full_protein_name', 'type', 'category', 'description', 'EC_number', 'begin', 'end']]\n",
    "\n",
    "    orthos = pd.read_table(cwd + '/orthologues/' + kind + '_orthologues.csv', sep=';')\n",
    "    orthos = list(zip(orthos[orthos['Species'] == 'Homo_sapiens'].values.tolist()[0][3:], orthos[orthos['Species'] == 'Homo_sapiens'].columns.tolist()[3:]))\n",
    "    orthos = pd.DataFrame(orthos, columns = ['ensembl_accession', 'orthogroup'])\n",
    "    df = pd.merge(df, orthos, on='ensembl_accession')\n",
    "    df = df[['accession', 'orthogroup', 'ensembl_accession', 'gene', 'full_protein_name', 'type', 'category', 'description', 'EC_number', 'begin', 'end']]\n",
    "\n",
    "    df.to_csv(cwd + '/uniprot/' + kind + '_filtered_features.csv')\n",
    "\n",
    "    # scrivo i range di posizioni che mi interessano (in questo caso -1 e +1 sia per begin che per end)\n",
    "    df = pd.read_table(cwd + '/uniprot/' + kind + '_filtered_features.csv', sep=',').drop(columns='Unnamed: 0').reset_index()\n",
    "    df['begin'] = df['begin'].apply(lambda x: None if '~' in str(x) else x)\n",
    "    df['end'] = df['end'].apply(lambda x: None if '~' in str(x) else x)\n",
    "    df['b+1'] = df['begin'].apply(lambda x: str(int(x)+1) if not x == None else x)\n",
    "    df['b-1'] = df['begin'].apply(lambda x: str(int(x)-1) if not x == None else x)\n",
    "    df['e+1'] = df['end'].apply(lambda x: str(int(x)+1) if not x == None else x)\n",
    "    df['e-1'] = df['end'].apply(lambda x: str(int(x)-1) if not x == None else x)\n",
    "    df['b+1'], df['b-1'] = df['b+1'].astype(float), df['b-1'].astype(float)\n",
    "    df['e+1'], df['e-1'] = df['e+1'].astype(float), df['e-1'].astype(float)\n",
    "    df['begin'], df['end'] = df['begin'].astype(float), df['end'].astype(float)\n",
    "\n",
    "    uniprot_list = df[['ensembl_accession', 'index', 'begin', 'b+1', 'b-1', 'end', 'e+1', 'e-1']].values.tolist()\n",
    "\n",
    "    positions = json.load(open(cwd + '/alignments/' + kind + '_alignments/alignments.json'))\n",
    "\n",
    "    # confronto le posizioni trovate in uniprot con quelle trovate mediante allineamenti\n",
    "    prova = []\n",
    "\n",
    "    for group in positions:\n",
    "        x = positions[group]\n",
    "        acc_A, acc_B = x['Accessions'][0], x['Accessions'][1]\n",
    "\n",
    "        for pos in x['Positions']:\n",
    "            x2 = x['Positions'][pos]\n",
    "            score = x['Positions'][pos]['Score']\n",
    "\n",
    "            if score > 1:\n",
    "                al_A, al_B = x2['alignment_A'], x2['alignment_B']\n",
    "                pos_A, pos_B = float(x2['Homo_position_A']), float(x2['Homo_position_B'])\n",
    "                res_A, res_B = x2['Homo_residue_A'], x2['homo_residue_B']\n",
    "\n",
    "                for line in uniprot_list:\n",
    "                    accession = line[0]\n",
    "                    index = line[1]\n",
    "                    begin, begin_plus_one, begin_minus_one = line[2], line[3], line[4]\n",
    "                    end, end_plus_one, end_minus_one = line[5], line[6], line[7]\n",
    "\n",
    "                    if accession == acc_A:\n",
    "\n",
    "                        if pos_A == begin or pos_A == begin_plus_one or pos_A == begin_minus_one or pos_A == end or pos_A == end_plus_one or pos_A == end_minus_one:\n",
    "                            prova.append([acc_A, index, pos_A, res_A, res_B, al_A, al_B, score])\n",
    "                        if pos_B == begin or pos_B == begin_plus_one or pos_B == begin_minus_one or pos_B == end or pos_B == end_plus_one or pos_B == end_minus_one:\n",
    "                            prova.append([acc_A, index, pos_B, res_A, res_B, al_A, al_B, score])    \n",
    "\n",
    "                    if accession == acc_B:\n",
    "                        if pos_A == begin or pos_A == begin_plus_one or pos_A == begin_minus_one or pos_A == end or pos_A == end_plus_one or pos_A == end_minus_one:\n",
    "                            prova.append([acc_B, index, pos_A, res_A, res_B, al_A, al_B, score])\n",
    "                        if pos_B == begin or pos_B == begin_plus_one or pos_B == begin_minus_one or pos_B == end or pos_B == end_plus_one or pos_B == end_minus_one:\n",
    "                            prova.append([acc_B, index, pos_B, res_A, res_B, al_A, al_B, score]) \n",
    "\n",
    "    positions_found = pd.DataFrame(prova, columns=['ensembl_accession', 'index', 'position_found', 'residue_A', 'residue_B', 'alignment_A', 'alignment_B', 'score'])\n",
    "\n",
    "    final_df = pd.merge(df, positions_found, on=['index', 'ensembl_accession']).drop_duplicates()\n",
    "\n",
    "    # recupero i full protein name di ensembl dal database (proteina in esame e partner)\n",
    "    database = json.load(open('database.json'))\n",
    "    orthos = pd.read_table(cwd + '/orthologues/' + kind + '_orthologues.csv', sep=';')\n",
    "    orthos = list(zip(orthos[orthos['Species'] == 'Homo_sapiens'].values.tolist()[0][3:], orthos[orthos['Species'] == 'Homo_sapiens'].columns.tolist()[3:]))\n",
    "    orthos = pd.DataFrame(orthos, columns = ['ensembl_accession', 'orthogroup'])\n",
    "    final_df = pd.merge(final_df, orthos, on=['ensembl_accession', 'orthogroup'])\n",
    "    orthos['full_name'] = orthos['ensembl_accession'].apply(lambda x: database.get(x)[7] if not database.get(x) == None else x)\n",
    "\n",
    "    final_df['full_name_ensembl_partner'] = final_df['orthogroup'].apply(lambda x: database.get(\n",
    "        orthos[orthos['orthogroup'] == re.split('A|B', x)[0] + {'A': 'B', 'B': 'A'}.get(x[-1])].values.tolist()[0][0])[7] if not database.get(\n",
    "        orthos[orthos['orthogroup'] == re.split('A|B', x)[0] + {'A': 'B', 'B': 'A'}.get(x[-1])].values.tolist()[0][0]) == None else None)\n",
    "\n",
    "    final_df['full_name_ensembl'] = final_df['orthogroup'].apply(lambda x: database.get(\n",
    "        orthos[orthos['orthogroup'] == x].values.tolist()[0][0])[7] if not database.get(\n",
    "        orthos[orthos['orthogroup'] == x].values.tolist()[0][0]) == None else None)\n",
    "\n",
    "    # ordino la tabella e la salvo in locale\n",
    "    final_df = final_df.drop(columns=['index', 'b+1', 'b-1', 'e+1', 'e-1']).drop_duplicates()[['orthogroup', 'accession', 'ensembl_accession', 'gene', 'full_protein_name', 'full_name_ensembl', 'full_name_ensembl_partner', 'type', 'category', 'description', 'begin', 'end', 'position_found', 'score', 'residue_A', 'residue_B', 'alignment_A', 'alignment_B', 'EC_number']]\n",
    "    final_df = final_df[~final_df['category'].str.contains('STRUCTURAL')]\n",
    "    final_df = final_df[~final_df['category'].str.contains('TOPOLOGY')]\n",
    "    final_df.to_csv(cwd + '/uniprot/' + kind + '_features.csv')\n",
    "\n",
    "features('Convergent')\n",
    "features('Divergent')\n",
    "features('Tandem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
