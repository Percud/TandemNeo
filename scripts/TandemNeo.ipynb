{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python3.7\n",
    "#anaconda3\n",
    "\n",
    "#conda create -n myenv3.7 python=3.7\n",
    "#conda activate myenv3.7 \n",
    "#conda install...\n",
    "\n",
    "#clustalo            0.1.2\n",
    "#blastp \n",
    "\n",
    "#biopython           1.78\n",
    "#ipykernel           5.3.4\n",
    "#ipython             7.19.0\n",
    "#ipython-genutils    0.2.0\n",
    "#json5               0.9.5\n",
    "#jupyterlab          2.2.6\n",
    "#matplotlib          3.3.2\n",
    "#natsort             7.1.0\n",
    "#numpy               1.19.2\n",
    "#pandas              1.1.5\n",
    "#pymol               2.4.1\n",
    "#requests            2.25.1\n",
    "#urllib3             1.26.2\n",
    "#xmltodict           0.12.0\n",
    "\n",
    "# informati sui sistemi docker\n",
    "# il setup.py dei pacchetti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules and config import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules and congig import\n",
    "\n",
    "import os, sys, gzip, json\n",
    "import pandas as pd\n",
    "from   multiprocessing import Pool, Manager\n",
    "from   itertools       import product\n",
    "from   Bio             import SeqIO\n",
    "\n",
    "from   specieinfo      import assemblies\n",
    "from   specieinfo      import specieinfo      as si\n",
    "from   download        import download        as dl\n",
    "from   mainisoforms    import mainisoforms    as mi\n",
    "from   duplications    import duplications    as dup\n",
    "from   orthology       import ortho           as ort \n",
    "from   faforalignments import faforalignments as ffal\n",
    "from   database        import database        as db\n",
    "from   database        import dbinfo\n",
    "from   alignments      import alignments      as al\n",
    "from   features        import features        as feat\n",
    "\n",
    "cwd = os.path.dirname(os.getcwd())\n",
    "sys.path.append(cwd)\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(cwd + '/appris')\n",
    "os.mkdir(cwd + '/alignments')\n",
    "os.mkdir(cwd + '/alignments/tandem')\n",
    "os.mkdir(cwd + '/alignments/divergent')\n",
    "os.mkdir(cwd + '/alignments/convergent')\n",
    "os.mkdir(cwd + '/blast_queries')\n",
    "os.mkdir(cwd + '/duplications')\n",
    "os.mkdir(cwd + '/fa')\n",
    "os.mkdir(cwd + '/gtf')\n",
    "os.mkdir(cwd + '/main')\n",
    "os.mkdir(cwd + '/orthologues')\n",
    "os.mkdir(cwd + '/features')\n",
    "os.mkdir(cwd + '/clustering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Species info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not have_list:\n",
    "\n",
    "    species = [a['name'] for a in assemblies['species']]       # collect all available species name in Ensembl database\n",
    "    slist = [si(s).allinfo for s in species]                   # specieinfo class for each of those species name\n",
    "\n",
    "    df = pd.DataFrame(slist, columns=[\n",
    "                                    'Class',\n",
    "                                    'Order',\n",
    "                                    'Genus',\n",
    "                                    'Specie',\n",
    "                                    'Publications',\n",
    "                                    'Taxid',\n",
    "                                    'Assembly']).dropna()      # writing a dataframe with all collected informations\n",
    "    df = df[df['Class'].isin(rf)]                              # filtering the dataframe for classes specified withing the config file\n",
    "    df['Publications'] = df['Publications'].astype(int)\n",
    "    df['Genus']        = df['Genus'].str.capitalize()          # capitalize genus value\n",
    "    df = df.sort_values('Publications', ascending=False)       # sorting for the publications\n",
    "    if mo:\n",
    "        df = df.groupby('Order').head(mo)                      # limiting the max number of orders to collect (specified in config)\n",
    "    if mg:\n",
    "        df = df.groupby('Genus', sort=False).head(mg)          # same for the genres\n",
    "    df = df.sort_values(['Class', 'Order'])                    # last sorting for the final dataframe\n",
    "    df.to_csv(cwd + '/species/species_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. FASTAs and GTFs download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = pd.read_csv(cwd + '/species/species_list.csv')            # open species containing file\n",
    "fa = [l[2] + '_' + l[3] for l in ls.values.tolist()]           # write a list containing all the specie names for the fasta files\n",
    "gtf = rs                                                       # write a list containing all the specie names for the gtf files\n",
    "\n",
    "pool = Pool(threads)                                           # activating the dl function in multiprocessing\n",
    "pool.starmap(dl.downl, product(gtf, ['gtf']))                  # starmap multiprocessing accepts tuple with arguments [('Homo_sapiens', 'gtf'), ('Gallus_gallus', 'gtf'), etc...]\n",
    "pool.starmap(dl.downl, product(fa, ['fa']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Main isoforms and BLASTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainblast(s, n, e, h, t):\n",
    "    mi.tofa(s)                                                 # activating tofa function, write a principal isoforms containing .fa file\n",
    "    mi.totsv(s)                                                # activating totsv function, write a principal isoforms, sorted over the genome, containing .tsv file\n",
    "    mi.blast(s, n, e, h, t)                                    # performing an intraspecie blastP\n",
    "\n",
    "blast_args = [num_threads,\n",
    "              evalue,\n",
    "              max_hsps,\n",
    "              max_target_seqs]\n",
    "\n",
    "pool = Pool(threads)                                           # activating the dl function in multiprocessing\n",
    "args = [tuple([s] + blast_args) for s in rs]\n",
    "pool.starmap(mainblast, args)                                  # starmap multiprocessing accepts tuple with arguments [('Homo_sapiens', 20, '10e-6', 1, 5), ('Gallus_gallus', 20, '10e-6', 1, 5), etc...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Duplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tocsv(s, k):                                               # saving a .tsv file containing a list of duplications filtered for specie e duplication kind\n",
    "\n",
    "    dups = dup.duplist(s, k)                                   # retrieving the accessions list for specie and duplication kind\n",
    "    for l in dups:\n",
    "        path = cwd + '/duplications/'\n",
    "        file = path + s + '_' + k + '.tsv'\n",
    "        print(*l, sep='\\t', file=open(file, 'a'))              # writing duplicated pair in a tsv file\n",
    "\n",
    "Pool(threads).starmap(tocsv, product(rs, dups))                # activating the tocsv function in multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Orthology (COMPARA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forquery(ID, d):\n",
    "    \n",
    "    file = d + '_no_filter.csv'\n",
    "    path = cwd + '/orthologues/' + file\n",
    "    df   = ort.df(ID[0], True)                                 # per ogni ID fornito scarica la lista degli ortologhi in formato dataframe, se viene fornita una lista di specie viene filtrato per queste\n",
    "    df['orthogroup'] = ID[1]                                   # assegna un numero corrispondende all'ortogruppo in base all'ordine di apparizione nel genoma dell'ID in esame\n",
    "    df   = df.values.tolist()\n",
    "\n",
    "    for l in df:\n",
    "        print(*l, sep=';', file=open(path, 'a'))               # dopo la conversione in lista delle righe del dataframe, vengono stampati al momento i risultati in un file con tutte le informazioni non filtrate\n",
    "        \n",
    "def forspecie(rs, d):\n",
    "        \n",
    "    dups_df = ort.mergedups(rs, d)[[8, 10]]                    # unisce i dataframe contenenti i geni duplicati di tutte le specie di riferimento per ciascun tipo di duplicazione\n",
    "    IDS = [l for l in dups_df.values.tolist()]                 # recupera tutti gli ID e i numeri degli ortogruppi assegnati per ogni tipo di duplicazione\n",
    "    Pool(5).starmap(forquery, product(IDS, [d]))               # attiva la funzione (in multiprocessing) forquery per ogni ID presente nel dataframe nato dall'unione precedente\n",
    "\n",
    "dups = ['convergent', 'divergent']\n",
    "for d in dups:\n",
    "    file = d + '_no_filter.csv'\n",
    "    path = cwd + '/orthologues/' + file\n",
    "    forspecie(rs, d)                                           # attiva la funzione for specie per ogni specie di riferimento e tipo di duplicazione\n",
    "    ort.brh(path)                                              # brh interno nel dataframe di ortologia ottenuto, altri commenti presenti in orthology.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### aggiungi a orthology\n",
    "\n",
    "database = json.load(open(cwd + '/database.json'))\n",
    "\n",
    "df = pd.read_table('/Users/carloderito/Desktop/TandemNeo/orthologues/tandem.csv', sep=';').set_index(['Class', 'Order', 'Species'])\n",
    "df = df.applymap(lambda x: x if str(x) == 'nan' or not x in database.keys() else database.get(x)['gene id'])\n",
    "df.to_csv('/Users/carloderito/Desktop/TandemNeo/orthologues/tandem_genes.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df    = db.orthodf('Tandem')                                   # opening orthologues dataframe for:\n",
    "slist = df['Species'].values.tolist()                          # obtaining species list\n",
    "\n",
    "manager = Manager()\n",
    "data    = manager.dict()\n",
    "\n",
    "def db_func(s):\n",
    " \n",
    "    path   = cwd + '/fa/' + s + '.fa.gz'\n",
    "    handle = gzip.open(path, 'rt')\n",
    "    fastas = list(SeqIO.parse(handle, \"fasta\"))                # opening file.fa --> storing in a list\n",
    "\n",
    "    orthos = db.aclist(s)                                      # accessions list from the orthologues tab\n",
    "    f = [l for l in fastas \n",
    "         if l.id.split('.')[0] in orthos]                      # intersection between orthologues accession list and fastas\n",
    "\n",
    "    for l in f:\n",
    "        data.update(db.info(l))                                # activating database class info function\n",
    "        \n",
    "Pool(threads).map(db_func, \n",
    "    [s for s in slist])\n",
    "        \n",
    "json.dump(data.copy(), open(cwd + '/database.json', 'w'))      # dumping json database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. FASTAs for alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dups:                                                 # for each kind of duplication\n",
    "\n",
    "    df    = db.orthodf(d)                                      # open and keep in memory the orthologues dataframe\n",
    "    pairs = ffal.pairslist(d)                                  # write a list containing the orthologues dataframe column indexes corresponding to orthogroup pairs \n",
    "    suff  = db.suffixes()                                      # write a dictionary containing the species references based on accessions Ensembl coding {ENSP0: 'Homo_sapiens'}\n",
    "    js    = json.load(open(cwd + '/database.json'))            # open the local database wrote in step 6\n",
    "    \n",
    "    for p in pairs:                                            # iterating over dataframe column indexes\n",
    "        ffal.printfa(df, p, suff, js, d)                       # writing FASTA file (1.fa will contain the FASTA corresponding to columns 1A and 1B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_proteome = pd.read_csv('https://www.uniprot.org/uniprot/?query=organism:9606&columns=id,entry%20name,reviewed,protein%20names,genes,organism,length,ec&format=tab', sep = '\\t')\n",
    "sums = []\n",
    "\n",
    "for d in dups:\n",
    "    \n",
    "    folder = 'alignments/' + d\n",
    "    ogroups = al.fanum(folder, '.fa')\n",
    "    \n",
    "    for o in ogroups:                                          # for each orthogroup\n",
    "        \n",
    "        fa   = 'alignments/' + d + '/' + str(o) + '.fa'        # FASTA file path\n",
    "        faln = 'alignments/' + d + '/' + str(o) + '.fasta'     # Aligned FASTA file path\n",
    "        al.clustifcov(fa, faln, th_coverage)                   # perform a clustalo if sequence lenghts coverage is higher than config threshold\n",
    "        \n",
    "    manager = Manager()\n",
    "    logs = manager.dict()\n",
    "    \n",
    "    def log(ogroup, srefs, matrix):\n",
    "        \n",
    "        try:\n",
    "            faln     = folder + '/' + str(ogroup) + '.fasta'   # Aligned FASTA file path\n",
    "            falnfile = al.alignmentfile(faln)                  # aligned FASTA file\n",
    "            if not ogroup in logs.keys():\n",
    "                log  = al.log(falnfile, srefs, matrix)         # return a log containing alignments informations\n",
    "                logs.update({ogroup: log})                     # storing those information inside a Manager dictionary for multiprocessing\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    Pool(threads).starmap(                                     # activating log function in multiprocessing\n",
    "        log, product(\n",
    "            ogroups, [srefs], [matrix]))\n",
    "        \n",
    "    json.dump(logs.copy(), open(\n",
    "        cwd + '/' + folder + '/' + d + '.json', 'w'))          # dumping those informations inside a json file\n",
    "    \n",
    "    f=al.threshold_aln(logs, str(alignment_threshold))\n",
    "    f['kind'] = d\n",
    "    score_1=feat.convert_ac(f['ensembl_ac_protein'].drop_duplicates().dropna().tolist(), 'ENSEMBL_PRO_ID', 'ACC').rename(columns={'From':'ensembl_ac_protein','To':'uniprot_ac'}).merge(f)\n",
    "    tabella_score_1_uniprot=pd.merge(human_proteome[['Entry','Gene names','Protein names','EC number']], score_1, left_on='Entry', right_on='uniprot_ac')\n",
    "    sums.append(tabella_score_1_uniprot.\n",
    "                astype({'ensembl_num':'float64'}).\n",
    "                sort_values(['total_scores>' + str(alignment_threshold), 'ensembl_num'], ascending=[False,True]).\n",
    "                drop('uniprot_ac', axis=1))\n",
    "\n",
    "pd.concat(sums).to_csv(cwd + '/alignments/sum_of_scores_' + str(alignment_threshold) + '.csv', sep=';',index=False)\n",
    "\n",
    "### DA RIPROVARE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aggiungi range delle posizioni nella tabella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dups:\n",
    "\n",
    "    ortholist = dbinfo.ids_to(                                 # IDs conversion from protein to gene \n",
    "        features_ref_specie, d, 'gene')\n",
    "    converted = feat.convert_id(                               # IDs conversion from ENSEMBL to Uniprot\n",
    "        'ENSEMBL_ID', 'ACC', ortholist)\n",
    "    converted_IDS = [v for k,v in converted.items()            # storing in a list all uniprot IDS \n",
    "                     if not 'ENS' in v]\n",
    "\n",
    "    manager = Manager()\n",
    "    appended_data = manager.list()\n",
    "    def getfeaturesparallel(k):\n",
    "        data = feat.getfeatures(k)                             # using features class getfeatures function to retrieve a complete features dataset for each ID\n",
    "        appended_data.append(data)\n",
    "    Pool(threads).map(getfeaturesparallel, converted_IDS)      # activating getfeaturesparallel function in multiprocessing\n",
    "\n",
    "    allfeatures = pd.concat(appended_data)                     # concatenated dataframe with features infos\n",
    "\n",
    "    path = cwd + '/alignments/' + d + '/'\n",
    "    aln = json.load(open(path + d + '.json'))                  # opening the json format alignments containing file\n",
    "    alns = al.threshold_aln(\n",
    "        aln, alignment_threshold)                              # set a threshold based on the alignment scores and return a dataframe\n",
    "    alns = feat.add_genes_ids(alns, converted)                 # add more infos in the dataframe\n",
    "    \n",
    "    features = feat.intersect_alns_features(\n",
    "        alns, allfeatures)                                     # return an intersection between alignments dataframe and allfeatures dataframe\n",
    "    # --> feat.filter(allfeatures)\n",
    "    features.to_csv(cwd + '/features/' + d + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### download_pdb\n",
    "\n",
    "os.chdir(cwd + '/clustering/')\n",
    "pdb_ca = get_coord_ca(clust_ref_specie)\n",
    "\n",
    "for d in dups:\n",
    "\n",
    "    clustering(pdb_ca,\n",
    "               clust_ref_specie + '/' + clust_ref_specie + '.csv', \n",
    "               json.load(open(cwd + '/alignments/' + d + '/' + d + '.json')),\n",
    "               alignment_threshold, \n",
    "               clust_mean_samples, \n",
    "               clust_eps)\n",
    "    \n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impor alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tabella generale con types e categories e da questa scegliamo per -inclusione- le voci che ci interessano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "              \n",
    "df = pd.read_table('/Users/carloderito/Desktop/TandemNeo/features/tandem.csv', sep=',')\n",
    "#df = df.drop(columns=['Unnamed: 0'])\n",
    "df[df['type'] == 'ACT_SITE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['description'] == '4-hydroxyproline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['category'] == 'PTM'].groupby('description').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['category'] == 'PTM']['description'].drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_table('/Users/carloderito/Downloads/features_threshold_1 - Features.csv', sep=',', header=None)\n",
    "df2[df2[9] == 'MUTAGENESIS'].drop_duplicates(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = cwd + '/alignments/' + d + '/'\n",
    "#x = json.load(open(path + d + '.json')) \n",
    "\n",
    "for aln in x.items():\n",
    "    if aln[0] == '27':\n",
    "        for p in x[aln[0]]['Positions'].items():\n",
    "            if p[1]['Score difference']>1:\n",
    "                for tag in p[1].items():\n",
    "                    #print(tag[0])\n",
    "                    if type(tag[1])== dict:\n",
    "                        for ac in p[1][tag[0]].keys():\n",
    "                            #if ac == 'ENSP00000226279':\n",
    "                                #if type(ac[1])== dict:\n",
    "                                #print(aln[0],p[0],tag[0],ac[0],ac[1]['Positions'],ac[1]['Residue'],tag[1]['Column'],p[1]['Score difference'],)\n",
    "                            print(tag, p[1]['Score difference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3.7",
   "language": "python",
   "name": "myenv3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
