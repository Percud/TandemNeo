{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TandemNeo_results import ort, stats, uni\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "cwd = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 • CONFIG •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 12. Statistics\n",
    "\n",
    "rf = ['Mammalia', 'Sauropsida', 'Actinopteri']             # select a reference classes list, None if you don't want to choose one           \n",
    "rs = ['Homo_sapiens', 'Gallus_gallus', 'Danio_rerio']      # select a reference species list\n",
    "du = ['tandem', 'convergent', 'divergent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthologues accessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort.whichdup('ENSP00000256999')                         # Return duplication kind for a given ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort.dataframe('ENSGALP00000072350')                        # Return given accession containing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort.pairs('ENSP00000256999')                            # Return pairs orthologues accession for a given ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort.orthodict('ENSGALP00000072350', rf, False)             # Return a columns containing dictionary, False if you don't want to include nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort.isclass('ENSGALP00000072350', 'Mammalia')              # Check the taxonomy class for a given ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort.whereisdup('ENSP00000256999', rf, 15)             # Check in which classes a given ID is duplicated, the float is the column populousness threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meglio non usare il linguaggio naturale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthologues dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ts()                                                 # orders per each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.mi('Homo_sapiens')                                   # main isoforms count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ai('Homo_sapiens')                                   # all isoforms count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.tc('Homo_sapiens', 'divergent')                         # duplicated genes count per duplication kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.tcdf(rs, du).applymap(lambda x: int(x/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats.wherdup('Tandem', rf, 0.15)\n",
    "stats.whichdup('Tandem', rf, 0.15)                         # duplication events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aim = stats.aim(rs).reset_index()                          # Main isoforms count\n",
    "tcdf = stats.tcdf(rs, du).reset_index()                    # Duplicated genes count\n",
    "qc = stats.bc(rs).T                                        # query / subjects ratio\n",
    "qc['blast ratios'] = qc['subjects'] / qc['queries']\n",
    "\n",
    "df = stats.allinfo(aim, tcdf).join(qc)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.alldups(0.15, rf)                                    # All duplication events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.outofmean('Tandem', 1000)                             # Return sequence lenghts outliers --> max-min/max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "riprendi quello degli allineamenti (coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prova tutto con altro pesce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "età duplicazione e orientamento duplicazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniprot features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of positions\n",
    "len(uni.all_scores('Divergent', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores median\n",
    "np.median(np.array(uni.all_scores('Divergent', 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores percentile\n",
    "np.quantile(np.array(uni.all_scores('Tandem', 0)), 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = uni.filtdf('Convergent', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['type'] == 'MUTAGEN'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all features count\n",
    "df = uni.allfeat(None).fillna(0)\n",
    "# mettere niente come argomento per il default?\n",
    "# aggiungere argomento range (+1-1)\n",
    "df['Sum'] = df['Tandem'] + df['Convergent'] + df['Divergent']\n",
    "df.astype(int)\n",
    "# prova tabella finale con \"duplication\" e \"unique gene\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ort:\n",
    "    \n",
    "    def whichdup(acc):\n",
    "\n",
    "        def read(kind):\n",
    "            path = cwd + '/orthologues/' + kind + '_orthologues.csv'\n",
    "            return pd.read_table(path, sep=';')\n",
    "\n",
    "        t = read('Tandem').isin([acc]).any()\n",
    "        d = read('Divergent').isin([acc]).any()\n",
    "        c = read('Convergent').isin([acc]).any()\n",
    "\n",
    "        if any(t):\n",
    "            return 'Tandem'\n",
    "        elif any(d):\n",
    "            return 'Divergent'\n",
    "        elif any(c):\n",
    "            return 'Convergent'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        #ort.whichdup('ENSGALP00000072350')\n",
    "        \n",
    "    def dataframe(acc):\n",
    "        kind = ort.whichdup(acc)\n",
    "        path = cwd + '/orthologues/' + kind + '_orthologues.csv'\n",
    "        df = pd.read_table(path, sep=';')\n",
    "        return df.set_index(['Classes', 'Orders', 'Species'])\n",
    "        \n",
    "        #ort.dataframe('ENSGALP00000072350')\n",
    "\n",
    "    def pairs(acc):\n",
    "        df = ort.dataframe(acc)\n",
    "        p = df.columns[df.isin([acc]).any()][0]\n",
    "        p_num = re.split('A|B', p)[0]\n",
    "\n",
    "        if 'B' in p:\n",
    "            pairs = [p_num + 'A', p]\n",
    "        else:\n",
    "            pairs = [p, p_num + 'B']\n",
    "\n",
    "        return pairs\n",
    "\n",
    "        #ort.pairs('ENSGALP00000072350')\n",
    "\n",
    "    def ortholist(acc, ref_classes, includenone):\n",
    "        df = ort.dataframe(acc)\n",
    "        df = df.loc[ref_classes]\n",
    "        df = df[ort.pairs(acc)].values.tolist()\n",
    "\n",
    "        if not includenone:\n",
    "            dfA = [l[0] for l in df \n",
    "                   if str(l[0]) != 'nan']\n",
    "            dfB = [l[1] for l in df \n",
    "                   if str(l[1]) != 'nan']\n",
    "        else:\n",
    "            dfA = [l[0] for l in df]\n",
    "            dfB = [l[1] for l in df]\n",
    "            \n",
    "        return [dfA, dfB]\n",
    "\n",
    "        #ort.ortholist('ENSGALP00000072350', 'Sauropsida', False)\n",
    "\n",
    "    def orthodict(acc, ref_classes, includenone):\n",
    "        df = ort.dataframe(acc)\n",
    "        c = {}\n",
    "        for line in ref_classes:\n",
    "            df2 = df[ort.pairs(acc)].loc[line].values.tolist()\n",
    "\n",
    "            if not includenone:\n",
    "                dfA = [l[0] for l in df2 \n",
    "                       if str(l[0]) != 'nan']\n",
    "                dfB = [l[1] for l in df2 \n",
    "                       if str(l[1]) != 'nan']\n",
    "            else:\n",
    "                dfA = [l[0] for l in df2]\n",
    "                dfB = [l[1] for l in df2]\n",
    "            c.update({line: [{'A': dfA}, {'B': dfB}]})\n",
    "        return c\n",
    "    \n",
    "        #ort.orthodict('ENSGALP00000072350', rf, True)\n",
    "\n",
    "    def isclass(acc, cla):\n",
    "        df = ort.dataframe(acc)\n",
    "        df = df[ort.pairs(acc)].loc[cla]\n",
    "        numna = len(df.values.tolist())\n",
    "        num = len(df.iloc[:,0].dropna().tolist())\n",
    "        if num > numna * 0.15:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        #ort.isclass('ENSGALP00000072350', 'Sauropsida')\n",
    "        \n",
    "    def whereisdup(ac, rf, t):\n",
    "\n",
    "        path = cwd + '/orthologues/' + ort.whichdup(ac) + '_orthologues.csv'\n",
    "        df = pd.read_table(path, sep=';')\n",
    "        df = df.set_index(['Classes', 'Orders', 'Species'])\n",
    "\n",
    "        m = df[ort.pairs(ac)].loc[rf[0]].count()\n",
    "        s = df[ort.pairs(ac)].loc[rf[1]].count()\n",
    "        a = df[ort.pairs(ac)].loc[rf[2]].count()\n",
    "\n",
    "        tsa = len(df.loc[rf[0]].index.tolist())\n",
    "        tsm = len(df.loc[rf[1]].index.tolist())\n",
    "        tss = len(df.loc[rf[2]].index.tolist())\n",
    "\n",
    "        if m.tolist()[0] > tsm*t and m.tolist()[1] > tsm*t:\n",
    "            ma = rf[0]\n",
    "        else:\n",
    "            ma = None\n",
    "\n",
    "        if s.tolist()[0] > tss*t and s.tolist()[1] > tss*t:\n",
    "            sa = rf[1]\n",
    "        else:\n",
    "            sa = None\n",
    "\n",
    "        if a.tolist()[0] > tsa*t and a.tolist()[1] > tsa*t:\n",
    "            ac = rf[2]\n",
    "        else:\n",
    "            ac = None\n",
    "\n",
    "        mat = {\n",
    "                str(['Mammalia', None, None]): 'Only Mammalia',\n",
    "                str([None, 'Sauropsida', None]): 'Only Sauropsida',\n",
    "                str([None, None, 'Actinopteri']): 'Only Actinopteri',\n",
    "                str(['Mammalia', 'Sauropsida', None]): 'Mammalia and Sauropsida',\n",
    "                str(['Mammalia', None, 'Actinopteri']): 'Mammalia and Actinopteri',\n",
    "                str([None, 'Sauropsida', 'Actinopteri']): 'Sauropsida and Actinopteri',\n",
    "                str(['Mammalia', 'Sauropsida', 'Actinopteri']): 'Mammalia, Sauropsida and Actinopteri',\n",
    "                str([None, None, None]): 'Have not accessions',\n",
    "                }\n",
    "\n",
    "        return mat.get(str([ma, sa, ac]))\n",
    "\n",
    "        #ort.whereisdup('ENSGALP00000072350', rf, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stats():\n",
    "\n",
    "    # species classes count\n",
    "    def ts():\n",
    "        path = 'species_list.txt'\n",
    "        s = pd.read_table(path, sep=' ', header=None)\n",
    "        df = pd.DataFrame(s.groupby(0).count()[1]).reset_index().T\n",
    "        df.columns = df.iloc[0]\n",
    "        return df[1:]\n",
    "\n",
    "        # stats.ts()\n",
    "\n",
    "    # all isoforms count\n",
    "    def ai(s):\n",
    "        a = []\n",
    "        path = cwd + '/fa/' + s + '.fa.gz'\n",
    "        fa = SeqIO.parse(gzip.open(path, 'rt'), 'fasta')\n",
    "        for fasta in fa:\n",
    "            if 'gene_biotype:protein_coding' in fasta.description:\n",
    "                a.append(fasta.id)\n",
    "        return len(a)\n",
    "\n",
    "        # stats.ai('Homo_sapiens')\n",
    "\n",
    "    # main isoforms count\n",
    "    def mi(s):\n",
    "        path = cwd + '/main/' + s + '.tsv'\n",
    "        df = pd.read_table(path, sep='\\t')\n",
    "        count = df['Protein_id'].count().tolist()\n",
    "        return count\n",
    "\n",
    "        # stats.mi('Homo_sapiens')\n",
    "        \n",
    "    # all and main isoforms ratio\n",
    "    def aim(rs):\n",
    "        mi = [stats.mi(x) for x in rs]\n",
    "        ai = [stats.ai(x) for x in rs]\n",
    "        index=['main', 'all']\n",
    "        df = pd.DataFrame([mi, ai], index=index, columns=rs).T\n",
    "        df['% main'] = df['main'] * 100 / df['all']\n",
    "        return df\n",
    "    \n",
    "        # stats.aim('Homo_sapiens')\n",
    "\n",
    "    # duplicated genes count\n",
    "    def tc(s, k):\n",
    "        path = cwd + '/tandem/' + s + '_' + k + '.tsv'\n",
    "        df = pd.read_table(path, sep='\\t', header=None)\n",
    "        df = df.values.tolist()\n",
    "        return len(df)*2\n",
    "    \n",
    "        # stats.tc('Homo_sapiens', 'tandem')\n",
    "        \n",
    "    def tcdf(rs, du):\n",
    "        tcdf = {\n",
    "                rs[0]: {\n",
    "                    du[0]: stats.tc(rs[0], du[0]), \n",
    "                    du[1]: stats.tc(rs[0], du[1]), \n",
    "                    du[2]: stats.tc(rs[0], du[2]),\n",
    "                    },\n",
    "                rs[1]: {\n",
    "                    du[0]: stats.tc(rs[1], du[0]), \n",
    "                    du[1]: stats.tc(rs[1], du[1]), \n",
    "                    du[2]: stats.tc(rs[1], du[2]),\n",
    "                    },\n",
    "                rs[2]: {\n",
    "                    du[0]: stats.tc(rs[2], du[0]), \n",
    "                    du[1]: stats.tc(rs[2], du[1]), \n",
    "                    du[2]: stats.tc(rs[2], du[2]),\n",
    "                    },\n",
    "        }\n",
    "\n",
    "        return pd.DataFrame(tcdf).T\n",
    "    \n",
    "        #stats.tcdf(rs, du)\n",
    "        \n",
    "    # return numbers and percentage about main species\n",
    "    def allinfo(df1, df2):\n",
    "        df = pd.merge(df1, df2, on='index')\n",
    "        df['% tandem'] = df['tandem'] * 100 / df['main']\n",
    "        df['% convergent'] = df['convergent'] * 100 / df['main']\n",
    "        df['% divergent'] = df['divergent'] * 100 / df['main']\n",
    "        df = df.set_index('index')\n",
    "        df.index.name = None\n",
    "        \n",
    "        return df\n",
    "    \n",
    "        # stats.allinfo(stats.aim(rs).reset_index(), stats.tcdf(rs, du).reset_index())\n",
    "\n",
    "    # blast comparisons count\n",
    "    def bcf(specie):\n",
    "        path = cwd + '/blast_queries/' + specie + '_main_blast.txt'\n",
    "        x = pd.read_table(path, comment='#', header=None)\n",
    "        x = x[x[2] != 100]\n",
    "        q = x.drop_duplicates(0)[0].count().tolist()\n",
    "        s = x[1].count().tolist()\n",
    "        return [q, s]\n",
    "    \n",
    "    def bc(rs):\n",
    "        df = pd.DataFrame({\n",
    "        rs[0]: {'queries': stats.bcf(rs[0])[0], \n",
    "                'subjects': stats.bcf(rs[0])[1]},\n",
    "        rs[1]: {'queries': stats.bcf(rs[1])[0], \n",
    "                'subjects': stats.bcf(rs[1])[1]},\n",
    "        rs[2]: {'queries': stats.bcf(rs[2])[0], \n",
    "                'subjects': stats.bcf(rs[2])[1]}})\n",
    "        return df\n",
    "\n",
    "        #stats.bc(rs)\n",
    "        \n",
    "    def wherdup(kind, rf, threshold):\n",
    "\n",
    "        def isdup(kind, c):\n",
    "            path = cwd + '/orthologues/' + kind + '_orthologues.csv'\n",
    "            df = pd.read_table(path, sep=';')\n",
    "            df = df.set_index(['Classes', 'Orders', 'Species'])\n",
    "\n",
    "            # total number of accessions\n",
    "            it = itertools.chain.from_iterable(df.values.tolist())\n",
    "            accessions = len([l for l in list(it) if not str(l) == 'nan'])\n",
    "\n",
    "            # if both columns sum is > 20%\n",
    "            df = df.applymap(lambda x: 1 if str(x) != 'nan' else 0).loc[c]\n",
    "            df = df.reset_index().drop(columns=['Orders', 'Species']).T\n",
    "            ts = int(df.columns.tolist()[-1])\n",
    "            df['sum'] = df.sum(axis=1)\n",
    "            df['Isdup'] = df['sum'].apply(lambda x: 1 if x > ts*threshold else 0) \n",
    "\n",
    "            def pairwise(iterable):\n",
    "                \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "                a = iter(iterable)\n",
    "                return zip(a, a)\n",
    "            lp = list(pairwise(df.index))\n",
    "\n",
    "            x = pd.DataFrame([df.loc[list(l)]['Isdup'].tolist() for l in lp])\n",
    "            x['Isdup_' + c] = x.sum(axis=1)  \n",
    "            x = x['Isdup_' + c].reset_index()\n",
    "\n",
    "            return x\n",
    "\n",
    "            #stats.isdup('Convergent', 'Actinopteri')\n",
    "\n",
    "        df_list = []\n",
    "        for x in range(len(rf)):\n",
    "            df_list.append(isdup(kind, rf[x]))\n",
    "\n",
    "        msa = reduce(lambda df1,df2: pd.merge(df1,df2,on='index'), df_list)\n",
    "        msa = msa.drop(columns='index')\n",
    "\n",
    "        return msa\n",
    "    \n",
    "        # stats.wherdup('Tandem', rf, 0.15)\n",
    "\n",
    "    def whichdup(kind, rf, threshold):\n",
    "\n",
    "        df = stats.wherdup(kind, rf, threshold)\n",
    "        df['m'] = df['Isdup_Mammalia'].apply(lambda x: 'Mammalia' if x == 2 else None)\n",
    "        df['s'] = df['Isdup_Sauropsida'].apply(lambda x: 'Sauropsida' if x == 2 else None)\n",
    "        df['a'] = df['Isdup_Actinopteri'].apply(lambda x: 'Actinopteri' if x == 2 else None)\n",
    "        df = df[['m', 's', 'a']]\n",
    "\n",
    "        mat = {\n",
    "                    str(['Mammalia', None, None]): 'Only Mammalia',\n",
    "                    str([None, 'Sauropsida', None]): 'Only Sauropsida',\n",
    "                    str([None, None, 'Actinopteri']): 'Only Actinopteri',\n",
    "                    str(['Mammalia', 'Sauropsida', None]): 'Mammalia and Sauropsida',\n",
    "                    str(['Mammalia', None, 'Actinopteri']): 'Mammalia and Actinopteri',\n",
    "                    str([None, 'Sauropsida', 'Actinopteri']): 'Sauropsida and Actinopteri',\n",
    "                    str(['Mammalia', 'Sauropsida', 'Actinopteri']): 'Mammalia, Sauropsida and Actinopteri',\n",
    "                    str([None, None, None]): 'Have not accessions',\n",
    "        }\n",
    "\n",
    "        df['msa'] = df.values.tolist()\n",
    "        df['which'] = df['msa'].apply(lambda x: mat.get(str(x)))\n",
    "        df = pd.DataFrame(df['which'].value_counts().to_dict(), index=[kind])\n",
    "\n",
    "\n",
    "        def addsum(df, k):\n",
    "            classes = [l for l in df.columns.tolist() if k in l]\n",
    "            df['Total ' + k] = df[classes].sum().sum()\n",
    "            return df\n",
    "\n",
    "        df = addsum(df, 'Mammalia')\n",
    "        df = addsum(df, 'Sauropsida')\n",
    "        df = addsum(df, 'Actinopteri')\n",
    "        df = df.T.sort_values(kind, ascending=False)\n",
    "\n",
    "        return df\n",
    "    \n",
    "        # stats.whichdup('Convergent', rf, 0.15)\n",
    "    \n",
    "    # All duplication events\n",
    "    def alldups(t, rf):\n",
    "        dfc = stats.whichdup('Convergent', rf, t).reset_index()\n",
    "        dft = stats.whichdup('Tandem', rf, t).reset_index()\n",
    "        dfd = stats.whichdup('Divergent', rf, t).reset_index()\n",
    "\n",
    "        dftc = pd.merge(dft, dfc, on='index')\n",
    "        df = pd.merge(dftc, dfd, on='index')\n",
    "        df = df.set_index('index')\n",
    "        df.index.name = None\n",
    "\n",
    "        return df\n",
    "    \n",
    "        #stats.alldups(0.15, rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class uni:\n",
    "\n",
    "    def all_scores(kind, threshold):\n",
    "\n",
    "        path = cwd + '/alignments/' + kind + '_alignments/'\n",
    "        file = 'alignments.json'\n",
    "        db = json.load(open(path + file))\n",
    "\n",
    "        scores = []\n",
    "        for l in db:\n",
    "            pos = db[l]['Positions']\n",
    "            for l1 in pos:\n",
    "                s = pos[l1]['Score']\n",
    "                if s > threshold:\n",
    "                    scores.append(s)\n",
    "\n",
    "        return scores\n",
    "    \n",
    "        #uni.all_scores('Convergent', 0)\n",
    "    \n",
    "    def filtdf(kind, threshold):\n",
    "\n",
    "        # scrivo i range di posizioni che mi interessano (in questo caso -1 e +1 sia per begin che per end)\n",
    "        df = pd.read_table(cwd + '/uniprot/' + kind + '_filtered_features.csv', sep=',').drop(columns='Unnamed: 0').reset_index()\n",
    "        df['begin'] = df['begin'].apply(lambda x: None if '~' in str(x) else x)\n",
    "        df['end'] = df['end'].apply(lambda x: None if '~' in str(x) else x)\n",
    "        df['b+1'] = df['begin'].apply(lambda x: str(int(x)+1) if not x == None else x)\n",
    "        df['b-1'] = df['begin'].apply(lambda x: str(int(x)-1) if not x == None else x)\n",
    "        df['e+1'] = df['end'].apply(lambda x: str(int(x)+1) if not x == None else x)\n",
    "        df['e-1'] = df['end'].apply(lambda x: str(int(x)-1) if not x == None else x)\n",
    "        df['b+1'], df['b-1'] = df['b+1'].astype(float), df['b-1'].astype(float)\n",
    "        df['e+1'], df['e-1'] = df['e+1'].astype(float), df['e-1'].astype(float)\n",
    "        df['begin'], df['end'] = df['begin'].astype(float), df['end'].astype(float)\n",
    "\n",
    "        uniprot_list = df[['ensembl_accession', 'index', 'begin', 'b+1', 'b-1', 'end', 'e+1', 'e-1']].values.tolist()\n",
    "\n",
    "        positions = json.load(open(cwd + '/alignments/' + kind + '_alignments/alignments.json'))\n",
    "\n",
    "        # confronto le posizioni trovate in uniprot con quelle trovate mediante allineamenti\n",
    "        prova = []\n",
    "\n",
    "        for group in positions:\n",
    "            x = positions[group]\n",
    "            acc_A, acc_B = x['Accessions'][0], x['Accessions'][1]\n",
    "\n",
    "            for pos in x['Positions']:\n",
    "                x2 = x['Positions'][pos]\n",
    "                score = x['Positions'][pos]['Score']\n",
    "\n",
    "                if score > threshold:\n",
    "                    al_A, al_B = x2['alignment_A'], x2['alignment_B']\n",
    "                    pos_A, pos_B = float(x2['Homo_position_A']), float(x2['Homo_position_B'])\n",
    "                    res_A, res_B = x2['Homo_residue_A'], x2['homo_residue_B']\n",
    "\n",
    "                    for line in uniprot_list:\n",
    "                        accession = line[0]\n",
    "                        index = line[1]\n",
    "                        begin, begin_plus_one, begin_minus_one = line[2], line[3], line[4]\n",
    "                        end, end_plus_one, end_minus_one = line[5], line[6], line[7]\n",
    "\n",
    "                        if accession == acc_A:\n",
    "\n",
    "                            if pos_A == begin or pos_A == begin_plus_one or pos_A == begin_minus_one or pos_A == end or pos_A == end_plus_one or pos_A == end_minus_one:\n",
    "                                prova.append([acc_A, index, pos_A, res_A, res_B, al_A, al_B, score])\n",
    "                            if pos_B == begin or pos_B == begin_plus_one or pos_B == begin_minus_one or pos_B == end or pos_B == end_plus_one or pos_B == end_minus_one:\n",
    "                                prova.append([acc_A, index, pos_B, res_A, res_B, al_A, al_B, score])    \n",
    "\n",
    "                        if accession == acc_B:\n",
    "                            if pos_A == begin or pos_A == begin_plus_one or pos_A == begin_minus_one or pos_A == end or pos_A == end_plus_one or pos_A == end_minus_one:\n",
    "                                prova.append([acc_B, index, pos_A, res_A, res_B, al_A, al_B, score])\n",
    "                            if pos_B == begin or pos_B == begin_plus_one or pos_B == begin_minus_one or pos_B == end or pos_B == end_plus_one or pos_B == end_minus_one:\n",
    "                                prova.append([acc_B, index, pos_B, res_A, res_B, al_A, al_B, score]) \n",
    "\n",
    "        positions_found = pd.DataFrame(prova, columns=['ensembl_accession', 'index', 'position_found', 'residue_A', 'residue_B', 'alignment_A', 'alignment_B', 'score'])\n",
    "\n",
    "        final_df = pd.merge(df, positions_found, on=['index', 'ensembl_accession']).drop_duplicates()\n",
    "\n",
    "        # recupero i full protein name di ensembl dal database (proteina in esame e partner)\n",
    "        database = json.load(open('database.json'))\n",
    "        orthos = pd.read_table(cwd + '/orthologues/' + kind + '_orthologues.csv', sep=';')\n",
    "        orthos = list(zip(orthos[orthos['Species'] == 'Homo_sapiens'].values.tolist()[0][3:], orthos[orthos['Species'] == 'Homo_sapiens'].columns.tolist()[3:]))\n",
    "        orthos = pd.DataFrame(orthos, columns = ['ensembl_accession', 'orthogroup'])\n",
    "        final_df = pd.merge(final_df, orthos, on=['ensembl_accession', 'orthogroup'])\n",
    "        orthos['full_name'] = orthos['ensembl_accession'].apply(lambda x: database.get(x)[7] if not database.get(x) == None else x)\n",
    "\n",
    "        final_df['full_name_ensembl_partner'] = final_df['orthogroup'].apply(lambda x: database.get(\n",
    "            orthos[orthos['orthogroup'] == re.split('A|B', x)[0] + {'A': 'B', 'B': 'A'}.get(x[-1])].values.tolist()[0][0])[7] if not database.get(\n",
    "            orthos[orthos['orthogroup'] == re.split('A|B', x)[0] + {'A': 'B', 'B': 'A'}.get(x[-1])].values.tolist()[0][0]) == None else None)\n",
    "\n",
    "        final_df['full_name_ensembl'] = final_df['orthogroup'].apply(lambda x: database.get(\n",
    "            orthos[orthos['orthogroup'] == x].values.tolist()[0][0])[7] if not database.get(\n",
    "            orthos[orthos['orthogroup'] == x].values.tolist()[0][0]) == None else None)\n",
    "\n",
    "        # ordino la tabella e la salvo in locale\n",
    "        final_df = final_df.drop(columns=['index', 'b+1', 'b-1', 'e+1', 'e-1']).drop_duplicates()[['orthogroup', 'accession', 'ensembl_accession', 'gene', 'full_protein_name', 'full_name_ensembl', 'full_name_ensembl_partner', 'type', 'category', 'description', 'begin', 'end', 'position_found', 'score', 'residue_A', 'residue_B', 'alignment_A', 'alignment_B', 'EC_number']]\n",
    "        final_df = final_df[~final_df['category'].str.contains('STRUCTURAL')]\n",
    "        final_df = final_df[~final_df['category'].str.contains('TOPOLOGY')]\n",
    "\n",
    "        return final_df\n",
    "\n",
    "        #uni.filtdf('Convergent', 0)\n",
    "        \n",
    "    def types(kind, threshold):\n",
    "\n",
    "        if threshold:\n",
    "            df = uni.filtdf(kind, threshold)\n",
    "        else:\n",
    "            df = pd.read_table(cwd + '/uniprot/' + kind + '_features.csv', sep=',')\n",
    "        types = df.groupby('type').count()['accession'].reset_index()\n",
    "        types = types.rename(columns={'accession': kind})\n",
    "        u_types = df.drop_duplicates(['type', 'accession'])\n",
    "        u_types = u_types.groupby('type').count()['accession'].reset_index()\n",
    "        u_types = u_types.rename(columns={'accession': 'Unique_' + kind.lower()})\n",
    "        df = pd.merge(types, u_types, on='type')\n",
    "\n",
    "        return df\n",
    "    \n",
    "        #uni.types('Convergent', 0)\n",
    "        \n",
    "    def allfeat(threshold):\n",
    "\n",
    "        p = pd.merge(uni.types('Tandem', threshold), uni.types('Convergent', threshold), on='type', how='outer')\n",
    "        df = pd.merge(p, uni.types('Divergent', threshold), on='type', how='outer').set_index('type')\n",
    "        df.index.name = None\n",
    "\n",
    "        return df\n",
    "\n",
    "        #uni.allfeat(None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
